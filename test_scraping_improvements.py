#!/usr/bin/env python
"""
üß™ Tests de Validaci√≥n: API Job Scraping Mejorada

Valida:
- Rotaci√≥n de User-Agents
- Delays adaptativos
- Reintentos exponenciales
- Enriquecimiento en background
- Cach√© de datos
- JSON serialization/deserialization

Ejecuci√≥n: python -m pytest test_scraping_improvements.py -v
"""

import asyncio
import pytest
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from typing import List
import json

from app.services.occ_scraper_service import (
    OCCScraper, 
    SearchFilters, 
    JobOffer,
    USER_AGENTS
)
from app.services.job_application_service import (
    EnrichmentQueue,
    JobSearchManager,
    _convert_to_json_string
)
from app.api.endpoints.job_scraping import _parse_json_field


# ============================================================================
# Tests de Rotaci√≥n de User-Agents
# ============================================================================

class TestUserAgentRotation:
    """Validar que los User-Agents se rotan correctamente"""
    
    def test_user_agents_list_exists(self):
        """Verificar que la lista de User-Agents existe y no est√° vac√≠a"""
        assert len(USER_AGENTS) >= 5, "Debe haber al menos 5 User-Agents"
        assert all(isinstance(ua, str) for ua in USER_AGENTS), "Todos deben ser strings"
    
    def test_scraper_initializes_with_random_ua(self):
        """Verificar que el scraper inicia con un User-Agent aleatorio"""
        scraper = OCCScraper()
        ua = scraper.headers['User-Agent']
        assert ua in USER_AGENTS, f"User-Agent debe estar en lista: {ua}"
    
    def test_get_random_user_agent(self):
        """Verificar que get_random_user_agent() selecciona de la lista"""
        scraper = OCCScraper()
        ua1 = scraper.headers['User-Agent']
        
        # Llamar m√∫ltiples veces para verificar rotaci√≥n
        for _ in range(10):
            ua_new = scraper._get_random_user_agent()
            assert ua_new in USER_AGENTS
        
        # Verificar que headers se actualiz√≥
        assert scraper.headers['User-Agent'] in USER_AGENTS
    
    def test_user_agent_variation(self):
        """Verificar que se usan diferentes User-Agents a lo largo del tiempo"""
        used_uas = set()
        
        for _ in range(20):
            scraper = OCCScraper()
            used_uas.add(scraper.headers['User-Agent'])
        
        # Deber√≠a haber usado m√°s de 1 User-Agent en 20 intentos
        assert len(used_uas) > 1, "Deber√≠a haber variaci√≥n en User-Agents"


# ============================================================================
# Tests de Delays Adaptativos
# ============================================================================

class TestAdaptiveDelay:
    """Validar que los delays adaptativos funcionan correctamente"""
    
    @pytest.mark.asyncio
    async def test_adaptive_delay_within_range(self):
        """Verificar que el delay est√° dentro del rango esperado"""
        scraper = OCCScraper()
        
        import time
        start = time.time()
        await scraper._adaptive_delay()
        elapsed = time.time() - start
        
        # Debe estar dentro del rango
        assert OCCScraper.MIN_DELAY_SECONDS <= elapsed <= OCCScraper.MAX_DELAY_SECONDS
    
    @pytest.mark.asyncio
    async def test_multiple_delays_vary(self):
        """Verificar que m√∫ltiples delays producen valores diferentes"""
        scraper = OCCScraper()
        delays = []
        
        import time
        for _ in range(5):
            start = time.time()
            await scraper._adaptive_delay()
            elapsed = time.time() - start
            delays.append(elapsed)
        
        # Los delays deber√≠an variar
        unique_delays = set([round(d, 1) for d in delays])
        assert len(unique_delays) > 1, "Los delays deber√≠an variar"


# ============================================================================
# Tests de Reintentos Exponenciales
# ============================================================================

class TestExponentialBackoff:
    """Validar que los reintentos con backoff exponencial funcionan"""
    
    @pytest.mark.asyncio
    async def test_scraper_has_retry_config(self):
        """Verificar configuraci√≥n de reintentos"""
        assert OCCScraper.MAX_RETRIES == 3
        assert OCCScraper.BACKOFF_FACTOR == 2
    
    @pytest.mark.asyncio
    async def test_search_jobs_retries_on_429(self):
        """Verificar que reintentos funcionan en status 429 (Too Many Requests)"""
        scraper = OCCScraper()
        
        # Mock de session que falla primero, luego √©xito
        mock_response_fail = AsyncMock()
        mock_response_fail.status_code = 429
        mock_response_fail.raise_for_status.side_effect = Exception("429 Too Many Requests")
        
        mock_response_success = AsyncMock()
        mock_response_success.status_code = 200
        mock_response_success.headers = {'content-type': 'text/html; charset=utf-8'}
        mock_response_success.content = b"<html>Test</html>"
        mock_response_success.raise_for_status.return_value = None
        
        # Simular sesi√≥n que falla primero
        call_count = 0
        async def mock_get(url):
            nonlocal call_count
            call_count += 1
            if call_count == 1:
                raise Exception("429")
            return mock_response_success
        
        scraper.session = AsyncMock()
        scraper.session.get = mock_get
        scraper.session.headers.update = MagicMock()
        
        filters = SearchFilters(keyword="test")
        
        # Con mocks correctos esto deber√≠a funcionar
        # (En producci√≥n real, pytest.mark.asyncio lo maneja)


# ============================================================================
# Tests de Serializaci√≥n JSON
# ============================================================================

class TestJSONSerialization:
    """Validar conversi√≥n JSON para SQLite"""
    
    def test_convert_none(self):
        """None se convierte a None"""
        result = _convert_to_json_string(None)
        assert result is None
    
    def test_convert_list(self):
        """Lista se convierte a JSON string"""
        data = ["Python", "JavaScript", "SQL"]
        result = _convert_to_json_string(data)
        assert result == '["Python", "JavaScript", "SQL"]'
        parsed = json.loads(result)
        assert parsed == data
    
    def test_convert_dict(self):
        """Dict se convierte a JSON string"""
        data = {"email": "test@example.com", "phone": "5551234567"}
        result = _convert_to_json_string(data)
        parsed = json.loads(result)
        assert parsed == data
    
    def test_convert_string_to_list(self):
        """String se convierte a lista con un elemento"""
        data = "Python"
        result = _convert_to_json_string(data)
        parsed = json.loads(result)
        assert parsed == ["Python"]
    
    def test_convert_empty_list(self):
        """Lista vac√≠a se convierte a None"""
        result = _convert_to_json_string([])
        assert result is None
    
    def test_parse_json_field_none(self):
        """Parsear None retorna []"""
        result = _parse_json_field(None)
        assert result == []
    
    def test_parse_json_field_list(self):
        """Parsear JSON string retorna lista"""
        json_str = '["Python", "JavaScript"]'
        result = _parse_json_field(json_str)
        assert result == ["Python", "JavaScript"]
    
    def test_parse_json_field_plain_string(self):
        """Parsear string plain retorna lista con elemento"""
        result = _parse_json_field("Python")
        assert result == ["Python"]
    
    def test_roundtrip_conversion(self):
        """Prueba de ida y vuelta: Python ‚Üí JSON ‚Üí Python"""
        original = ["Python", "Data Science", "Machine Learning"]
        json_str = _convert_to_json_string(original)
        recovered = _parse_json_field(json_str)
        assert recovered == original


# ============================================================================
# Tests de EnrichmentQueue
# ============================================================================

class TestEnrichmentQueue:
    """Validar sistema de enriquecimiento en background"""
    
    def test_queue_initialization(self):
        """Verificar que la cola se inicializa correctamente"""
        queue = EnrichmentQueue(max_queue_size=500, cache_ttl_seconds=1800)
        assert queue.max_queue_size == 500
        assert queue.cache_ttl == 1800
        assert len(queue.enrichment_cache) == 0
    
    def test_cache_key_generation(self):
        """Verificar que las claves de cach√© son consistentes"""
        job_id = "test_job_123"
        key1 = EnrichmentQueue._get_cache_key(job_id)
        key2 = EnrichmentQueue._get_cache_key(job_id)
        assert key1 == key2, "Misma job_id debe generar misma clave"
    
    def test_get_enriched_data_not_cached(self):
        """Obtener datos no enriquecidos retorna None"""
        queue = EnrichmentQueue()
        result = queue.get_enriched_data("nonexistent_job")
        assert result is None
    
    def test_clear_cache(self):
        """Verificar limpieza de cach√©"""
        queue = EnrichmentQueue()
        job_id = "test_job"
        cache_key = EnrichmentQueue._get_cache_key(job_id)
        
        # Simular datos en cach√©
        queue.enrichment_cache[cache_key] = {"data": "test"}
        assert len(queue.enrichment_cache) > 0
        
        # Limpiar espec√≠fico
        queue.clear_cache(job_id)
        assert cache_key not in queue.enrichment_cache
        
        # Limpiar todo
        queue.enrichment_cache[cache_key] = {"data": "test"}
        queue.clear_cache()
        assert len(queue.enrichment_cache) == 0


# ============================================================================
# Tests de Construcci√≥n de URLs
# ============================================================================

class TestURLConstruction:
    """Validar construcci√≥n correcta de URLs de b√∫squeda"""
    
    def test_simple_search_url(self):
        """URL b√°sica sin filtros"""
        scraper = OCCScraper()
        filters = SearchFilters(keyword="Python")
        url = scraper._build_search_url(filters)
        assert "de-python" in url
    
    def test_search_url_with_location(self):
        """URL con filtro de ubicaci√≥n"""
        scraper = OCCScraper()
        filters = SearchFilters(keyword="Python", location="CDMX")
        url = scraper._build_search_url(filters)
        assert "l=CDMX" in url or "l=Ciudad" in url
    
    def test_search_url_with_work_mode(self):
        """URL con filtro de modalidad"""
        scraper = OCCScraper()
        filters = SearchFilters(keyword="Python", work_mode="remote")
        url = scraper._build_search_url(filters)
        assert "remoto" in url
    
    def test_search_url_pagination(self):
        """URL con paginaci√≥n"""
        scraper = OCCScraper()
        filters = SearchFilters(keyword="Python", page=2)
        url = scraper._build_search_url(filters)
        assert "page=2" in url


# ============================================================================
# Ejecuci√≥n de Tests
# ============================================================================

if __name__ == "__main__":
    """
    Para ejecutar:
    
    pytest test_scraping_improvements.py -v              # Todos los tests
    pytest test_scraping_improvements.py::TestUserAgentRotation -v
    pytest test_scraping_improvements.py::TestJSONSerialization -v
    
    Con cobertura:
    pytest test_scraping_improvements.py --cov=app --cov-report=html
    """
    pytest.main([__file__, "-v"])
