"""
Endpoints de la API para el servicio de scraping de OCC.com.mx
"""

from typing import Dict, List, Optional
from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel, Field
from datetime import datetime
import logging
from sqlmodel import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.services.occ_scraper_service import (
    SearchFilters,
    JobOffer,
    search_jobs_service,
    get_job_details_service,
    monitor_user_interests,
    OCCJobTracker,
    OCCScraper
)
from app.core.database import get_session
from app.models.job_scraping import (
    JobApplicationDB,
    SearchQueryDB,
    UserJobAlertDB
)
from app.models import JobPosition
from app.services.job_application_service import (
    JobApplicationManager,
    JobSearchManager,
    JobAlertManager,
    JobCacheManager
)
from app.middleware.auth import get_current_user

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/job-scraping", tags=["Job Scraping"])


# ============================================================================
# ESQUEMAS DE RESPUESTA Y REQUEST
# ============================================================================

class SearchRequest(BaseModel):
    """Request model para b√∫squeda de empleos"""
    keyword: str
    location: Optional[str] = None
    category: Optional[str] = None
    salary_min: Optional[int] = None  # Agregado campo salary_min
    salary_range: Optional[str] = None
    experience_level: Optional[str] = None
    work_mode: Optional[str] = None  # remote, hybrid, onsite
    job_type: Optional[str] = None
    company_verified: bool = False
    sort_by: str = "relevance"  # relevance, date
    page: int = 1


class SearchResponse(BaseModel):
    """Response model para resultados de b√∫squeda"""
    jobs: List[JobOffer]
    total_results: int
    current_page: int
    search_filters: Dict
    success: bool = True
    message: str = "B√∫squeda completada exitosamente"


class JobTrackingRequest(BaseModel):
    """Request model para rastreo de empleos"""
    keywords: List[str]
    location: Optional[str] = None
    max_pages: int = 3
    user_id: Optional[str] = None


class MonitoringResponse(BaseModel):
    """Response model para monitoreo de keywords"""
    user_id: Optional[str]
    monitored_keywords: List[str]
    results: Dict[str, List[JobOffer]]
    timestamp: str
    total_jobs_found: int
    success: bool = True


class ApplicationResponse(BaseModel):
    """Respuesta de aplicaci√≥n creada"""
    application_id: int
    job_title: str
    company: str
    status: str
    applied_at: datetime


class AlertResponse(BaseModel):
    """Respuesta de alerta creada"""
    alert_id: int
    keywords: List[str]
    frequency: str
    is_active: bool
    created_at: datetime


class StatsResponse(BaseModel):
    """Estad√≠sticas de usuario"""
    total_applications: int
    status_breakdown: Dict[str, int]
    recent_applications: int
    success_rate: float


# ============================================================================
# CACHE MODELS
# ============================================================================

class CacheStoreRequest(BaseModel):
    """Request para guardar resultados de b√∫squeda en cache persistente"""
    jobs: List[JobOffer]
    keyword: str
    source: str = "occ"


class CacheJobItem(BaseModel):
    """Item de empleo desde cache (sin la estructura completa de JobOffer)"""
    id: Optional[int] = None
    title: str
    company: str
    location: str
    description: str
    job_type: Optional[str] = None
    work_mode: Optional[str] = None
    experience_level: Optional[str] = None
    skills: Optional[str] = None
    salary_range: Optional[str] = None
    external_job_id: Optional[str] = None
    source: str
    scraped_at: datetime
    is_active: bool


class CacheListResponse(BaseModel):
    """Response de lista de empleos desde cache"""
    jobs: List[CacheJobItem]
    total: int
    from_cache: bool = True
    returned: int
    filters_applied: Dict = {}
    cache_age_minutes: Optional[int] = None
    message: str = "Datos obtenidos desde cache persistente"


class CacheStoreResponse(BaseModel):
    """Response al guardar empleos en cache"""
    saved_count: int
    total_cached: int
    source: str
    keyword: str
    message: str


class CacheStatsResponse(BaseModel):
    """Estad√≠sticas del cache persistente"""
    total_active: int
    expired_but_active: int
    soft_deleted: int
    total_db_records: int
    avg_age_hours: float
    top_locations: Dict[str, int]
    cache_efficiency: float  # % de registros activos vs total
    next_cleanup_recommended: bool
    timestamp: str


class JobApplicationRequest(BaseModel):
    """Request para crear aplicaci√≥n"""
    job_id: str = Field(..., description="ID del trabajo en OCC")
    external_url: Optional[str] = Field(None, description="URL de aplicaci√≥n externa")
    notes: Optional[str] = Field(None, description="Notas personales")


class JobAlertRequest(BaseModel):
    """Request para crear alerta de empleo"""
    keywords: List[str] = Field(..., min_items=1, description="Palabras clave para alertas")
    location: Optional[str] = Field(None, description="Ubicaci√≥n")
    salary_min: Optional[int] = Field(None, description="Salario m√≠nimo")
    work_mode: Optional[str] = Field(None, description="Modalidad de trabajo")
    frequency: str = Field("daily", description="Frecuencia de notificaciones")


class DetailedJobResponse(BaseModel):
    """Response mejorado para detalles de un empleo"""
    job_details: JobOffer
    extraction_quality: Dict = {}
    available_sections: Dict = {}
    recommendations: List[str] = []
    success: bool = True


# ============================================================================
# ENDPOINTS DE B√öSQUEDA Y SCRAPING
# ============================================================================


@router.post("/search", response_model=SearchResponse)
async def search_jobs(
    request: SearchRequest, 
    detailed: bool = Query(False, description="Incluir informaci√≥n enriquecida del contenedor"),
    full_details: bool = Query(False, description="Obtener detalles completos v√≠a API OCC (m√°s lento, 95%+ datos)"),
    db_session: AsyncSession = Depends(get_session)
):
    """
    Busca empleos en OCC.com.mx basado en los criterios especificados.
    
    ‚ú® NUEVO: Autom√°ticamente guarda resultados en cach√© persistente (BD)
    despu√©s de la b√∫squeda, para evitar rescraping en futuras consultas.
    
    Par√°metros:
    - detailed (query): Si es true, incluye informaci√≥n enriquecida del contenedor
    - full_details (query): Si es true, obtiene detalles COMPLETOS v√≠a API para cada job
                           (100-200ms por job adicional, recomendado usar cach√©)
    
    Ejemplos:
    - POST /api/v1/job-scraping/search?detailed=false (r√°pido, datos b√°sicos)
    - POST /api/v1/job-scraping/search?detailed=true (moderado, datos enriquecidos ~50%)
    - POST /api/v1/job-scraping/search?detailed=true&full_details=true (lento, datos completos ~95%)
    
    Nota: full_details implica detailed=true
    """
    try:
        # Convertir salary_min a salary_range si es necesario
        salary_range = request.salary_range
        if request.salary_min and not salary_range:
            if request.salary_min >= 50000:
                salary_range = "50000+"
            elif request.salary_min >= 30000:
                salary_range = "30000-50000"
            elif request.salary_min >= 20000:
                salary_range = "20000-30000"
            else:
                salary_range = "0-20000"
        
        # Convertir request a SearchFilters
        filters = SearchFilters(
            keyword=request.keyword,
            location=request.location,
            category=request.category,
            salary_range=salary_range,
            experience_level=request.experience_level,
            work_mode=request.work_mode,
            job_type=request.job_type,
            company_verified=request.company_verified,
            sort_by=request.sort_by,
            page=request.page
        )
        
        # Realizar b√∫squeda
        if detailed or full_details:
            async with OCCScraper() as scraper:
                jobs, total_results = await scraper.search_jobs_with_details(
                    filters, 
                    include_details=True,
                    fetch_full_details=full_details  # ‚Üê NUEVO PAR√ÅMETRO
                )
        else:
            jobs, total_results = await search_jobs_service(filters)
        
        # ‚ú® NUEVO: Guardar autom√°ticamente en cach√© persistente (BD)
        try:
            cache_manager = JobCacheManager(db_session)
            cached_count = await cache_manager.save_scraped_jobs(
                jobs=jobs,
                source="occ",
                keyword=request.keyword
            )
            logger.info(f"‚úÖ Cache: {cached_count} empleos guardados para keyword '{request.keyword}'")
        except Exception as cache_error:
            # No fallar la b√∫squeda si hay error en cache
            logger.warning(f"‚ö†Ô∏è  Error guardando cache: {cache_error}")
            cached_count = 0
        
        # Construir mensaje descriptivo
        if full_details:
            search_type = "completa v√≠a API (95%+ datos)"
        elif detailed:
            search_type = "enriquecida (~50% datos)"
        else:
            search_type = "est√°ndar (datos b√°sicos)"
        
        return SearchResponse(
            jobs=jobs,
            total_results=total_results,
            current_page=request.page,
            search_filters=filters.dict(),
            message=f"B√∫squeda {search_type} completada exitosamente. {cached_count if 'cached_count' in locals() else 'N/A'} empleos en cache."
        )
        
    except ImportError as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error de dependencias del scraper: {str(e)}"
        )
    except Exception as e:
        import traceback
        print(f"Error detallado en search_jobs: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"Error interno en la b√∫squeda: {str(e)}"
        )




class DetailedJobResponse(BaseModel):
    """Response mejorado para detalles de un empleo"""
    job_details: JobOffer
    extraction_quality: Dict = {}
    available_sections: Dict = {}
    recommendations: List[str] = []
    success: bool = True


@router.get("/job/{job_id}", response_model=DetailedJobResponse)
async def get_job_details(job_id: str):
    """
    Obtiene los detalles completos de una oferta de trabajo espec√≠fica.
    
    ‚ö†Ô∏è NOTA: OCC.com.mx tiene protecci√≥n anti-bot en las p√°ginas de detalles individuales.
    Este endpoint proporciona enriquecimiento de datos mediante b√∫squeda contextual.
    
    Para obtener detalles enriquecidos:
    1. Busca empleos con /search usando criterios (keyword, location, etc)
    2. Obt√©n los job_id del resultado
    3. Usa este endpoint para obtener la versi√≥n enriquecida
    
    O alternativamente, usa /search?detailed=true para b√∫squeda con extracci√≥n enriquecida.
    """
    try:
        # Intentar obtener detalles del sitio (probablemente fallar√° por anti-bot)
        job_details = await get_job_details_service(job_id)
        
        if not job_details:
            # Si falla, retornar un objeto vac√≠o con recomendaci√≥n
            logger.warning(f"No se pudieron obtener detalles para {job_id} via OCC - Protecci√≥n anti-bot detectada")
            
            # Crear un objeto JobOffer con valores por defecto y campos opcionales
            job_details = JobOffer(
                job_id=job_id,
                title="Detalles no disponibles directamente",
                company="No disponible",
                location="No disponible",
                publication_date=None,  # Expl√≠citamente None (ahora es opcional)
                url=f"https://www.occ.com.mx/empleo/{job_id}",  # Proporcionar URL construida
                description="OCC.com.mx tiene protecci√≥n anti-bot que impide obtener detalles directamente. Use /search?detailed=true para enriquecimiento."
            )
        
        # Analizar la calidad de la extracci√≥n
        extraction_quality = {
            "has_title": bool(job_details.title and job_details.title != "Sin t√≠tulo"),
            "has_company": bool(job_details.company and job_details.company != "Empresa no especificada"),
            "has_salary": bool(job_details.salary),
            "has_benefits": bool(job_details.benefits),
            "has_category": bool(job_details.category),
            "has_description": bool(job_details.full_description),
            "has_skills": bool(job_details.skills),
            "completeness_score": 0
        }
        
        # Calcular puntuaci√≥n de completitud
        completeness_fields = [
            job_details.title, job_details.company, job_details.location,
            job_details.salary, job_details.full_description, job_details.category
        ]
        extraction_quality["completeness_score"] = round(
            (sum(1 for f in completeness_fields if f) / len(completeness_fields)) * 100, 2
        )
        
        # Secciones disponibles
        available_sections = {
            "basic_info": True,
            "salary_info": bool(job_details.salary),
            "benefits": len(job_details.benefits) > 0,
            "job_category": bool(job_details.category),
            "job_requirements": bool(job_details.education_required),
            "technical_skills": len(job_details.skills) > 0,
            "soft_skills": len(job_details.soft_skills) > 0,
            "activities": len(job_details.activities) > 0,
            "contact_info": bool(job_details.contact_info),
            "full_description": bool(job_details.full_description)
        }
        
        # Recomendaciones
        is_data_limited = (job_details.title == "Detalles no disponibles directamente")
        
        if is_data_limited:
            recommendations = [
                "‚ö†Ô∏è OCC.com.mx tiene protecci√≥n anti-bot en p√°ginas individuales",
                "üí° Usa /search?detailed=true para obtener datos enriquecidos",
                f"üîó URL directa: https://www.occ.com.mx/empleo/{job_id}",
                "üìç Accede directamente al sitio para ver todos los detalles"
            ]
        else:
            recommendations = [
                "üí° Para obtener detalles enriquecidos: usa /search?detailed=true con b√∫squeda de empleo",
                f"üîó URL de OCC.com.mx: https://www.occ.com.mx/empleo/{job_id}"
            ]
        
        if not job_details.salary and not is_data_limited:
            recommendations.append("üí∞ El salario debe ser negociado directamente con el reclutador")
        if not job_details.full_description and not is_data_limited:
            recommendations.append("üìù Visita OCCMundial para ver la descripci√≥n completa del puesto")
        if len(job_details.benefits) == 0 and not is_data_limited:
            recommendations.append("üéÅ No se encontraron beneficios especificados en la oferta")
        if not job_details.contact_info and not is_data_limited:
            recommendations.append("üìû Contacta directamente a trav√©s del formulario en OCCMundial")
        
        return DetailedJobResponse(
            job_details=job_details,
            extraction_quality=extraction_quality,
            available_sections=available_sections,
            recommendations=recommendations
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"Error en get_job_details: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"Error al obtener detalles del empleo: {str(e)}"
        )


@router.post("/track", response_model=MonitoringResponse)
async def track_job_opportunities(
    request: JobTrackingRequest,
    session: AsyncSession = Depends(get_session)
):
    """
    Rastrea oportunidades laborales para m√∫ltiples keywords
    """
    try:
        tracker = OCCJobTracker(session)
        results = await tracker.monitor_keywords(request.keywords)
        
        # Contar total de empleos encontrados
        total_jobs = sum(len(jobs) for jobs in results.values())
        
        return MonitoringResponse(
            user_id=request.user_id,
            monitored_keywords=request.keywords,
            results=results,
            timestamp=datetime.now().isoformat(),
            total_jobs_found=total_jobs
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error al rastrear empleos: {str(e)}"
        )


@router.get("/trending-jobs", tags=["Discovery"])
async def get_trending_jobs(
    location: Optional[str] = Query(None, description="Ubicaci√≥n para filtrar empleos"),
    limit: int = Query(20, description="N√∫mero m√°ximo de empleos a retornar"),
    detailed: bool = Query(False, description="Incluir informaci√≥n detallada de cada empleo")
):
    """
    Obtiene los empleos m√°s populares/recientes de diferentes categor√≠as.
    
    Este endpoint realiza scraping en vivo de keywords predeterminadas para mostrar
    las tendencias actuales del mercado laboral.
    
    Par√°metros:
    - detailed (query): Si es true, incluye informaci√≥n enriquecida (categor√≠a, beneficios, skills, etc.)
    - limit: N√∫mero m√°ximo de empleos a retornar
    - location: Filtrar por ubicaci√≥n (opcional)
    
    Ejemplos:
    - GET /api/v1/job-scraping/trending-jobs
    - GET /api/v1/job-scraping/trending-jobs?detailed=true&limit=30&location=Mexico
    
    ‚úÖ ENDPOINT CONSOLIDADO - Una √∫nica fuente de verdad para tendencias en vivo
    """
    try:
        trending_keywords = [
            "data science",
            "python",
            "javascript", 
            "react",
            "an√°lisis de datos",
            "machine learning",
            "desarrollador web",
            "marketing digital"
        ]
        
        all_trending_jobs = []
        
        for keyword in trending_keywords[:4]:  # Limitar para no sobrecargar
            filters = SearchFilters(
                keyword=keyword,
                location=location,
                sort_by="date",
                page=1
            )
            
            if detailed:
                async with OCCScraper() as scraper:
                    jobs, _ = await scraper.search_jobs_with_details(filters, include_details=True)
            else:
                jobs, _ = await search_jobs_service(filters)
            
            # Tomar solo los primeros 3 empleos de cada categor√≠a
            category_jobs = jobs[:3]
            for job in category_jobs:
                job.category = keyword  # A√±adir categor√≠a para referencia
                
            all_trending_jobs.extend(category_jobs)
        
        # Limitar al n√∫mero solicitado
        return {
            "trending_jobs": all_trending_jobs[:limit],
            "total_found": len(all_trending_jobs),
            "keywords_searched": trending_keywords[:4],
            "detailed_info_included": detailed,
            "note": "Datos en vivo obtenidos por scraping. Recomendaci√≥n: cachear por 1-2 horas",
            "success": True
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error al obtener empleos trending: {str(e)}"
        )


@router.post("/monitor-user/{user_id}")
async def setup_user_monitoring(
    user_id: str,
    keywords: List[str],
    location: Optional[str] = None
):
    """
    Configura monitoreo personalizado para un usuario espec√≠fico
    """
    try:
        results = await monitor_user_interests(user_id, keywords)
        
        return {
            "message": f"Monitoreo configurado para usuario {user_id}",
            "monitoring_setup": results,
            "success": True
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error al configurar monitoreo: {str(e)}"
        )


@router.get("/search-suggestions")
async def get_search_suggestions(query: str = Query(..., description="T√©rmino de b√∫squeda parcial")):
    """
    Proporciona sugerencias de b√∫squeda basadas en t√©rminos populares
    """
    suggestions = {
        "data": [
            "data science",
            "data analyst",
            "data engineer", 
            "database administrator"
        ],
        "python": [
            "python developer",
            "python backend",
            "python django",
            "python flask"
        ],
        "web": [
            "web developer",
            "web designer",
            "web frontend",
            "web full stack"
        ],
        "marketing": [
            "marketing digital",
            "marketing manager",
            "marketing analytics",
            "marketing specialist"
        ]
    }
    
    query_lower = query.lower()
    matching_suggestions = []
    
    for category, terms in suggestions.items():
        if query_lower in category:
            matching_suggestions.extend(terms)
    
    # Si no hay coincidencias por categor√≠a, buscar en todos los t√©rminos
    if not matching_suggestions:
        all_terms = [term for terms_list in suggestions.values() for term in terms_list]
        matching_suggestions = [term for term in all_terms if query_lower in term.lower()]
    
    return {
        "query": query,
        "suggestions": matching_suggestions[:10],  # Limitar a 10 sugerencias
        "success": True
    }






class DetailedJobResponse(BaseModel):
    """Response mejorado para detalles de un empleo"""
    job_details: JobOffer
    extraction_quality: Dict = {}
    available_sections: Dict = {}
    recommendations: List[str] = []
    success: bool = True


@router.get("/statistics-advanced")
async def get_advanced_job_statistics(
    keywords: List[str] = Query(["data science", "python", "javascript", "marketing digital"], 
                                description="Keywords para analizar estad√≠sticas")
):
    """
    Proporciona estad√≠sticas avanzadas sobre empleos incluyendo an√°lisis de tendencias
    """
    try:
        async with OCCScraper() as scraper:
            stats = await scraper.get_job_statistics(keywords)
        
        return {
            **stats,
            "success": True,
            "message": f"Estad√≠sticas generadas para {len(keywords)} keywords"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error al generar estad√≠sticas avanzadas: {str(e)}"
        )


@router.get("/explore-site-structure")
async def explore_occ_site_structure(keyword: str = "data science"):
    """
    Endpoint de exploraci√≥n para entender la estructura del sitio OCC.com.mx
    """
    try:
        filters = SearchFilters(keyword=keyword, page=1)
        
        async with OCCScraper() as scraper:
            # Obtener URL de b√∫squeda
            search_url = scraper._build_search_url(filters)
            
            # Realizar una b√∫squeda para obtener ejemplos
            jobs, total = await scraper.search_jobs(filters)
            
            # Analizar el primer empleo si existe
            sample_analysis = None
            if jobs:
                first_job = jobs[0]
                sample_analysis = {
                    "job_id": first_job.job_id,
                    "title": first_job.title,
                    "has_all_basic_fields": all([
                        first_job.title,
                        first_job.company,
                        first_job.location,
                        first_job.publication_date
                    ]),
                    "available_fields": {
                        "salary": bool(first_job.salary),
                        "benefits": bool(first_job.benefits),
                        "company_verified": first_job.company_verified,
                        "is_featured": first_job.is_featured,
                        "is_new": first_job.is_new,
                        "company_logo": bool(first_job.company_logo)
                    }
                }
        
        return {
            "search_url_pattern": search_url,
            "total_results_found": total,
            "jobs_extracted": len(jobs),
            "sample_job_analysis": sample_analysis,
            "site_elements_identified": {
                "job_containers": "[data-offers-grid-offer-item-container]",
                "detail_container": "#job-detail-container",
                "title_selector": "[data-offers-grid-detail-title]",
                "company_verification": "svg verification icons",
                "benefits_list": "ul.list-disc.list-inside",
                "skills_data": "input#hd_skills",
                "contact_info": "input#hd_contact_*"
            },
            "extraction_capabilities": {
                "basic_info": "‚úÖ T√≠tulo, empresa, ubicaci√≥n, fecha",
                "detailed_info": "‚úÖ Categor√≠a, subcategor√≠a, educaci√≥n",
                "benefits": "‚úÖ Lista de beneficios estructurada",
                "skills": "‚úÖ Habilidades desde datos JSON ocultos",
                "work_details": "‚úÖ Modalidad, tipo de contrato",
                "contact": "‚úÖ Informaci√≥n de contacto cuando disponible"
            },
            "success": True,
            "message": f"Exploraci√≥n completada para keyword '{keyword}'"
        }
        
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error en exploraci√≥n del sitio: {str(e)}"
        )


# ============================================================================
# ENDPOINTS DE GESTI√ìN DE APLICACIONES
# ============================================================================

@router.post("/apply", response_model=ApplicationResponse, tags=["Applications"])
async def create_job_application(
    application_request: JobApplicationRequest,
    current_user = Depends(get_current_user),
    db_session: AsyncSession = Depends(get_session)
):
    """
    Crear una nueva aplicaci√≥n de empleo
    
    Registra que el usuario ha aplicado a un empleo espec√≠fico.
    """
    try:
        # Buscar la oferta de trabajo
        result = await db_session.execute(
            select(JobPosition).where(
                JobPosition.external_job_id == application_request.job_id
            )
        )
        job_offer = result.scalars().first()
        
        if not job_offer:
            raise HTTPException(status_code=404, detail="Oferta de empleo no encontrada")
        
        # Crear aplicaci√≥n
        app_manager = JobApplicationManager(db_session)
        application = app_manager.create_application(
            user_id=current_user.user_id,
            job_position_id=job_offer.id,
            external_url=application_request.external_url,
            notes=application_request.notes
        )
        
        return ApplicationResponse(
            application_id=application.id,
            job_title=job_offer.title,
            company=job_offer.company,
            status=application.status,
            applied_at=application.created_at
        )
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error creando aplicaci√≥n: {e}")
        raise HTTPException(status_code=500, detail="Error al crear aplicaci√≥n")


@router.get("/applications", tags=["Applications"])
async def get_user_applications(
    status: Optional[str] = Query(None, description="Filtrar por estado"),
    current_user = Depends(get_current_user),
    db_session = Depends(get_session)
):
    """
    Obtener las aplicaciones del usuario actual
    
    Lista todas las aplicaciones de empleo del usuario, opcionalmente filtradas por estado.
    """
    try:
        app_manager = JobApplicationManager(db_session)
        applications = app_manager.get_user_applications(
            user_id=current_user.user_id,
            status=status
        )
        
        return {
            "applications": applications,
            "total": len(applications)
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo aplicaciones: {e}")
        raise HTTPException(status_code=500, detail="Error al obtener aplicaciones")


@router.put("/application/{application_id}/status", tags=["Applications"])
async def update_application_status(
    application_id: int,
    status: str,
    notes: Optional[str] = None,
    current_user = Depends(get_current_user),
    db_session = Depends(get_session)
):
    """
    Actualizar el estatus de una aplicaci√≥n
    
    Permite cambiar el estado de una aplicaci√≥n (pendiente, entrevista, rechazado, etc.)
    """
    try:
        # Validar estados permitidos
        valid_statuses = ["applied", "pending", "interview", "rejected", "accepted", "withdrawn"]
        if status not in valid_statuses:
            raise HTTPException(
                status_code=400, 
                detail=f"Estado inv√°lido. Estados v√°lidos: {valid_statuses}"
            )
        
        app_manager = JobApplicationManager(db_session)
        application = app_manager.update_application_status(
            application_id=application_id,
            status=status,
            notes=notes
        )
        
        return {"message": "Estado actualizado exitosamente", "application": application}
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Error actualizando aplicaci√≥n: {e}")
        raise HTTPException(status_code=500, detail="Error al actualizar aplicaci√≥n")


@router.get("/applications/stats", response_model=StatsResponse, tags=["Applications"])
async def get_application_statistics(
    current_user = Depends(get_current_user),
    db_session = Depends(get_session)
):
    """
    Obtener estad√≠sticas de aplicaciones del usuario
    
    Proporciona m√©tricas sobre las aplicaciones de empleo del usuario.
    """
    try:
        app_manager = JobApplicationManager(db_session)
        stats = app_manager.get_application_statistics(current_user.user_id)
        
        return StatsResponse(**stats)
        
    except Exception as e:
        logger.error(f"Error obteniendo estad√≠sticas: {e}")
        raise HTTPException(status_code=500, detail="Error al obtener estad√≠sticas")


# ============================================================================
# ENDPOINTS DE ALERTAS DE EMPLEO
# ============================================================================

@router.post("/alerts", response_model=AlertResponse, tags=["Alerts"])
async def create_job_alert(
    alert_request: JobAlertRequest,
    current_user = Depends(get_current_user),
    db_session = Depends(get_session)
):
    """
    Crear una alerta de empleo
    
    Configura una alerta autom√°tica que notificar√° al usuario cuando aparezcan nuevos empleos
    que coincidan con sus criterios.
    """
    try:
        alert_manager = JobAlertManager(db_session)
        alert = alert_manager.create_job_alert(
            user_id=current_user.user_id,
            keywords=alert_request.keywords,
            location=alert_request.location,
            salary_min=alert_request.salary_min,
            work_mode=alert_request.work_mode,
            frequency=alert_request.frequency
        )
        
        return AlertResponse(
            alert_id=alert.id,
            keywords=alert.keywords,
            frequency=alert.frequency,
            is_active=alert.is_active,
            created_at=alert.created_at
        )
        
    except Exception as e:
        logger.error(f"Error creando alerta: {e}")
        raise HTTPException(status_code=500, detail="Error al crear alerta")


@router.get("/alerts", tags=["Alerts"])
async def get_user_alerts(
    current_user = Depends(get_current_user),
    db_session: AsyncSession = Depends(get_session)
):
    """
    Obtener alertas de empleo del usuario
    
    Lista todas las alertas configuradas por el usuario.
    """
    try:
        result = await db_session.execute(
            select(UserJobAlertDB).where(
                UserJobAlertDB.user_id == current_user.user_id
            )
        )
        alerts = result.scalars().all()
        
        return {"alerts": alerts, "total": len(alerts)}
        
    except Exception as e:
        logger.error(f"Error obteniendo alertas: {e}")
        raise HTTPException(status_code=500, detail="Error al obtener alertas")


@router.delete("/alerts/{alert_id}", tags=["Alerts"])
async def delete_job_alert(
    alert_id: int,
    current_user = Depends(get_current_user),
    db_session: AsyncSession = Depends(get_session)
):
    """
    Eliminar una alerta de empleo
    
    Desactiva o elimina una alerta espec√≠fica del usuario.
    """
    try:
        result = await db_session.execute(
            select(UserJobAlertDB).where(
                (UserJobAlertDB.id == alert_id) &
                (UserJobAlertDB.user_id == current_user.user_id)
            )
        )
        alert = result.scalars().first()
        
        if not alert:
            raise HTTPException(status_code=404, detail="Alerta no encontrada")
        
        alert.is_active = False
        await db_session.commit()
        
        return {"message": "Alerta desactivada exitosamente"}
        
    except Exception as e:
        logger.error(f"Error eliminando alerta: {e}")
        raise HTTPException(status_code=500, detail="Error al eliminar alerta")


@router.get("/search-history", tags=["Search"])
async def get_search_history(
    limit: int = Query(10, ge=1, le=50, description="N√∫mero de b√∫squedas a mostrar"),
    current_user = Depends(get_current_user),
    db_session = Depends(get_session)
):
    """
    Obtener historial de b√∫squedas del usuario
    
    Muestra las b√∫squedas recientes realizadas por el usuario.
    """
    try:
        search_manager = JobSearchManager(db_session)
        history = search_manager.get_search_history(
            user_id=current_user.user_id,
            limit=limit
        )
        
        return {
            "search_history": history,
            "total": len(history)
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo historial: {e}")
        raise HTTPException(status_code=500, detail="Error al obtener historial")


# ============================================================================
# CACHE MANAGEMENT ENDPOINTS
# ============================================================================

@router.post("/cache/store", response_model=CacheStoreResponse, tags=["Cache"])
async def store_search_results(
    request: CacheStoreRequest,
    db_session: AsyncSession = Depends(get_session)
):
    """
    Guarda resultados de b√∫squeda en cach√© persistente (BD).
    
    Normalmente llamado autom√°ticamente despu√©s de /search,
    pero tambi√©n puede ser llamado manualmente desde frontend.
    
    Deduplicaci√≥n:
    - Si el empleo ya existe (por external_job_id), lo actualiza
    - Si es nuevo, lo inserta
    
    TTL: 7 d√≠as por defecto
    
    ‚ú® MEJORADO: Validaci√≥n de entrada y mejor manejo de errores
    
    Args:
        jobs: Lista de empleos del scraper
        keyword: Palabra clave de b√∫squeda (para logging)
        source: Fuente del empleo (default: "occ")
        
    Returns:
        Cantidad de empleos guardados y total en cache
    """
    try:
        # ‚úÖ Validaci√≥n de entrada
        if not request.jobs or len(request.jobs) == 0:
            logger.warning(f"‚ö†Ô∏è  Intentando guardar lista vac√≠a de empleos (keyword: {request.keyword})")
            return CacheStoreResponse(
                saved_count=0,
                total_cached=0,
                source=request.source,
                keyword=request.keyword,
                message="‚ö†Ô∏è Lista de empleos vac√≠a, nada que guardar"
            )
        
        logger.info(f"üíæ Guardando {len(request.jobs)} empleos en cache (keyword: '{request.keyword}')")
        
        cache_manager = JobCacheManager(db_session)
        saved_count = await cache_manager.save_scraped_jobs(
            jobs=request.jobs,
            source=request.source,
            keyword=request.keyword
        )
        
        # ‚úÖ Obtener total en cache despu√©s de guardar
        try:
            stats = await cache_manager.get_cache_stats()
            total_cached = stats.get("total_active", 0)
        except Exception as stats_error:
            logger.warning(f"‚ö†Ô∏è  Error al obtener estad√≠sticas de cache: {stats_error}")
            total_cached = saved_count  # Usar al menos el conteo actual
        
        logger.info(f"‚úÖ Cache guardado: {saved_count} nuevos/actualizados, {total_cached} total activos")
        
        return CacheStoreResponse(
            saved_count=saved_count,
            total_cached=total_cached,
            source=request.source,
            keyword=request.keyword,
            message=f"‚úÖ {saved_count} empleos guardados en cache. Total en cache: {total_cached}"
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error guardando resultados en cache: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Error al guardar en cache: {str(e)}"
        )


@router.get("/cache/list", response_model=CacheListResponse, tags=["Cache"])
async def get_cached_jobs(
    location: Optional[str] = Query(None, description="Filtrar por ubicaci√≥n (b√∫squeda parcial)"),
    work_mode: Optional[str] = Query(None, description="Modalidad: presencial, remoto, h√≠brido"),
    experience_level: Optional[str] = Query(None, description="Nivel de experiencia requerido"),
    skills: Optional[str] = Query(None, description="Habilidades (b√∫squeda en descripci√≥n)"),
    job_type: Optional[str] = Query(None, description="Tipo de trabajo: full-time, part-time, etc."),
    sort_by: str = Query("recent", description="Ordenamiento: recent, relevance"),
    limit: int = Query(50, ge=1, le=200, description="M√°ximo de resultados"),
    offset: int = Query(0, ge=0, description="Saltar N resultados"),
    db_session: AsyncSession = Depends(get_session)
):
    """
    Obtiene empleos desde cach√© persistente con filtros opcionales.
    
    Esta es la forma principal de cargar empleos sin hacer scraping.
    Ideal para:
    - Cargar datos iniciales en /oportunidades
    - Aplicar filtros sin consultar scraper
    - Explorar empleos en cach√©
    
    Filtros soportados:
    - location: Ubicaci√≥n (b√∫squeda parcial, case-insensitive)
    - work_mode: Modalidad de trabajo
    - experience_level: Nivel de experiencia
    - skills: Habilidades (b√∫squeda en JSON de skills)
    - job_type: Tipo de contrato
    
    Ordenamiento:
    - recent: Por fecha de scraping (m√°s nuevo primero)
    - relevance: Por coincidencia de filtros (TODO)
    
    Returns:
        Lista de empleos, total en cache, y metadata
    """
    try:
        cache_manager = JobCacheManager(db_session)
        
        # Construir filtros
        filters = {}
        if location:
            filters["location"] = location
        if work_mode:
            filters["work_mode"] = work_mode
        if experience_level:
            filters["experience_level"] = experience_level
        if skills:
            filters["skills"] = skills
        if job_type:
            filters["job_type"] = job_type
        
        # Obtener empleos
        jobs, total = await cache_manager.get_cached_jobs(
            filters=filters,
            limit=limit,
            offset=offset
        )
        
        # Convertir a response
        cache_items = [
            CacheJobItem(
                id=job.id,
                title=job.title,
                company=job.company,
                location=job.location,
                description=job.description,
                job_type=job.job_type,
                work_mode=job.work_mode,
                experience_level=job.experience_level,
                skills=job.skills,
                salary_range=job.salary_range,
                external_job_id=job.external_job_id,
                source=job.source,
                scraped_at=job.scraped_at,
                is_active=job.is_active
            )
            for job in jobs
        ]
        
        # Calcular edad del cache m√°s antiguo
        if jobs:
            oldest_job = min(jobs, key=lambda j: j.scraped_at)
            cache_age_minutes = int((datetime.utcnow() - oldest_job.scraped_at).total_seconds() / 60)
        else:
            cache_age_minutes = None
        
        return CacheListResponse(
            jobs=cache_items,
            total=total,
            returned=len(cache_items),
            filters_applied=filters,
            cache_age_minutes=cache_age_minutes
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo cache: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error al obtener cache: {str(e)}"
        )


@router.post("/cache/invalidate", response_model=dict, tags=["Cache"])
async def invalidate_expired_jobs(
    max_age_days: int = Query(7, ge=1, le=90, description="Invalidar empleos con edad > N d√≠as"),
    db_session: AsyncSession = Depends(get_session)
):
    """
    Invalida (soft-delete) empleos expirados del cache.
    
    Estrategia de limpieza:
    - Soft-delete: marca is_active=False en lugar de borrar
    - Permite auditor√≠a y recuperaci√≥n si es necesario
    - Puede ejecutarse peri√≥dicamente (ej: cada 6 horas)
    
    Args:
        max_age_days: Invalidar empleos m√°s antiguos que N d√≠as (default: 7)
        
    Returns:
        Cantidad de empleos invalidados y fecha de pr√≥xima limpieza recomendada
    """
    try:
        cache_manager = JobCacheManager(db_session)
        invalidated_count = await cache_manager.invalidate_expired_jobs(max_age_days=max_age_days)
        
        # Obtener stats despu√©s de invalidaci√≥n
        stats = await cache_manager.get_cache_stats()
        
        return {
            "invalidated_count": invalidated_count,
            "total_active_remaining": stats["total_active"],
            "soft_deleted_total": stats["soft_deleted"],
            "message": f"‚ôªÔ∏è  {invalidated_count} empleos invalidados. Cache activo: {stats['total_active']}",
            "stats": stats
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error invalidando cache: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error al invalidar cache: {str(e)}"
        )


@router.get("/cache/stats", response_model=CacheStatsResponse, tags=["Cache"])
async def get_cache_statistics(
    db_session: AsyncSession = Depends(get_session)
):
    """
    Retorna estad√≠sticas del cach√© persistente.
    
    Informaci√≥n √∫til:
    - total_active: Empleos activos y no expirados
    - expired_but_active: Empleos que pasaron expires_at pero a√∫n activos
    - soft_deleted: Empleos marcados como inactivos
    - cache_efficiency: % de registros √∫tiles vs total en BD
    - avg_age_hours: Edad promedio del cache
    - top_locations: Top 5 ubicaciones con m√°s empleos
    - next_cleanup_recommended: Si debe ejecutarse /cache/invalidate
    
    Returns:
        Estad√≠sticas del cache para monitoreo y toma de decisiones
    """
    try:
        cache_manager = JobCacheManager(db_session)
        stats = await cache_manager.get_cache_stats()
        
        return CacheStatsResponse(**stats)
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo estad√≠sticas del cache: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error al obtener estad√≠sticas: {str(e)}"
        )


# ============================================================================
# ENDPOINTS ADMINISTRATIVOS
# ============================================================================

@router.post("/admin/process-alerts", tags=["Admin"])
async def process_job_alerts(
    # current_admin = Depends(get_admin_user),  # Requiere permisos de admin
    db_session = Depends(get_session)
):
    """
    Procesar todas las alertas de empleo (Endpoint administrativo)
    
    Verifica todas las alertas activas y env√≠a notificaciones cuando corresponda.
    Solo disponible para administradores.
    """
    try:
        alert_manager = JobAlertManager(db_session)
        results = await alert_manager.check_alerts_and_notify()
        
        return {
            "message": "Procesamiento de alertas completado",
            "results": results,
            "processed_at": datetime.utcnow()
        }
        
    except Exception as e:
        logger.error(f"Error procesando alertas: {e}")
        raise HTTPException(status_code=500, detail="Error al procesar alertas")