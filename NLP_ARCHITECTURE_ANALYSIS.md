# ğŸ§  AnÃ¡lisis de Arquitectura NLP: _extract_resume_analysis vs text_vectorization_service

## ğŸ“‹ Tabla de Contenidos
1. [RelaciÃ³n Actual](#relaciÃ³n-actual)
2. [Flujo de Datos](#flujo-de-datos)
3. [Independencia vs Acoplamiento](#independencia-vs-acoplamiento)
4. [Oportunidades con spaCy](#oportunidades-con-spacy)
5. [Arquitectura Propuesta](#arquitectura-propuesta)
6. [Roadmap de ImplementaciÃ³n](#roadmap-de-implementaciÃ³n)

---

## ğŸ”— RelaciÃ³n Actual

### `_extract_resume_analysis()` - Â¿Dependiente o Independiente?

**RESPUESTA CORTA**: âœ… **SEMI-INDEPENDIENTE** (ambos usan el mismo pipeline base)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUJO DE EXTRACCIÃ“N DE CV                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  1ï¸âƒ£ upload_resume() endpoint (app/api/endpoints/students.py)    â”‚
â”‚     â””â”€ Recibe: CV en PDF/DOCX/TXT + metadatos                   â”‚
â”‚                                                                   â”‚
â”‚  2ï¸âƒ£ extract_text_from_upload_async()                            â”‚
â”‚     â””â”€ Convierte PDF/DOCX â†’ texto plano                         â”‚
â”‚     â””â”€ Output: resume_text (str)                                â”‚
â”‚                                                                   â”‚
â”‚  3ï¸âƒ£ _extract_resume_analysis(resume_text)  â—€â”€â”€â”€ ğŸ“ AQUÃ         â”‚
â”‚     â”œâ”€ Llama: text_vectorization_service.analyze_document()     â”‚
â”‚     â”‚  â””â”€ Procesa: normalizaciÃ³n, tokenizaciÃ³n, anÃ¡lisis        â”‚
â”‚     â”‚  â””â”€ Retorna: Dict[technical_terms, soft_skills, ...]     â”‚
â”‚     â”‚                                                            â”‚
â”‚     â””â”€ Extrae: skills, soft_skills, projects                   â”‚
â”‚     â””â”€ Output: {"skills": [], "soft_skills": [], ...}          â”‚
â”‚                                                                   â”‚
â”‚  4ï¸âƒ£ _extract_harvard_cv_fields(resume_text)  â—€â”€â”€â”€ ğŸ“ AQUÃ       â”‚
â”‚     â”œâ”€ Usa: Regex + keyword matching (independiente)            â”‚
â”‚     â””â”€ Extrae: objective, education, experience, ...            â”‚
â”‚     â””â”€ Output: {"objective": "", "education": [], ...}          â”‚
â”‚                                                                   â”‚
â”‚  5ï¸âƒ£ Guardar en BD + Retornar ResumeAnalysisResponse             â”‚
â”‚     â””â”€ Ambos resultados se guardan y retornan                   â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Llamadas Actuales

```python
# En _extract_resume_analysis() - lÃ­nea ~86 de students.py:
doc_analysis = text_vectorization_service.analyze_document(resume_text)
#                 â†‘
#                 SÃ­ usa text_vectorization_service

# En _extract_harvard_cv_fields() - lÃ­nea ~150 de students.py:
education_section_match = re.search(r'(educaciÃ³n|education|...)', text_lower, ...)
#                         â†‘
#                         NO usa text_vectorization_service (es independiente)
```

---

## ğŸ“Š Flujo de Datos Detallado

### LÃ­nea 1: NormalizaciÃ³n y TokenizaciÃ³n

```python
# text_vectorization_service.py - normalize_text()
def normalize_text(text, normalization_type=AGGRESSIVE):
    """
    PASO 1: MinÃºsculas
    PASO 2: Unicode normalization (remover acentos)
    PASO 3: Mapeo tÃ©cnico (C++ â†’ cpp, Node.js â†’ nodejs)
    PASO 4: Eliminar caracteres especiales
    PASO 5: Eliminar stopwords (AGGRESSIVE)
    PASO 6: Espacios mÃºltiples colapsados
    """
    return normalized_text
```

**CaracterÃ­sticas**:
- âœ… Stopwords removal (AGGRESSIVE)
- âœ… Technical mapping (C++, Node.js, etc)
- âœ… Unicode normalization
- âœ… ProtecciÃ³n DoS (MAX_TEXT_LEN = 50k caracteres)

---

### LÃ­nea 2: AnÃ¡lisis de Documentos

```python
# text_vectorization_service.py - analyze_document()
def analyze_document(text):
    """
    Retorna Dict con:
    - normalized_text: str
    - token_count: int
    - unique_tokens: int
    - tokens: List[str]
    - technical_terms: List[(term, relevance)]  â—€â”€â”€â”€ Extrae tÃ©rminos tÃ©cnicos
    - soft_skills: List[(skill, relevance)]     â—€â”€â”€â”€ Extrae soft skills
    - keyphrases: List[(phrase, score)]         â—€â”€â”€â”€ Extrae frases clave
    - text_length: int
    - normalized_length: int
    """
```

**Este mÃ©todo usa**:
```python
TermExtractor.extract_technical_terms(text)    # Basado en TECHNICAL_VOCAB
TermExtractor.extract_soft_skills(text)        # Basado en SOFT_SKILLS_VOCAB
TermExtractor.extract_keyphrases(text)         # N-gramas significativos
```

---

### LÃ­nea 3: ExtracciÃ³n de Habilidades en _extract_resume_analysis()

```python
# students.py - lÃ­nea ~86-120
def _extract_resume_analysis(resume_text):
    doc_analysis = text_vectorization_service.analyze_document(resume_text)
    
    # Extrae de doc_analysis:
    technical_terms = doc_analysis["technical_terms"]      # List[(term, relevance)]
    soft_skills_detected = doc_analysis["soft_skills"]     # List[(skill, relevance)]
    keyphrases = doc_analysis["keyphrases"]                # List[(phrase, score)]
    
    # Procesa y filtra:
    skills = [term[0] for term in technical_terms][:MAX_SKILLS_EXTRACTED]
    soft_skills = [skill[0] for skill in soft_skills_detected][:MAX_SOFT_SKILLS_EXTRACTED]
    projects = [phrase for phrase in keyphrases if matches_project_keywords(phrase)]
    
    return {
        "skills": skills,                    # â† A la BD
        "soft_skills": soft_skills,          # â† A la BD
        "projects": projects,                # â† A la BD
        "confidence": confidence_score
    }
```

---

### LÃ­nea 4: ExtracciÃ³n Harvard CV (Independiente)

```python
# students.py - lÃ­nea ~150-250
def _extract_harvard_cv_fields(resume_text):
    """
    âŒ NO DEPENDE de text_vectorization_service
    âœ… Usa REGEX + keyword matching directamente
    """
    
    # Busca secciones por keywords
    education_section_match = re.search(
        r'(educaciÃ³n|education|formaciÃ³n|training)[\s\n]+(.*?)(?:experiencia|experience|...)',
        text_lower, re.DOTALL | re.IGNORECASE
    )
    
    # Extrae aÃ±os
    year_match = re.search(r'(20\d{2}|19\d{2})', ' '.join(lines_in_block))
    
    # Parse manual de lÃ­neas
    edu_record = {
        "institution": lines_in_block[0],
        "degree": lines_in_block[1],
        "field_of_study": lines_in_block[2],
        "graduation_year": int(year_match.group(1))
    }
    
    return {
        "objective": objective,           # â† A la BD
        "education": education,           # â† A la BD
        "experience": experience,         # â† A la BD
        "certifications": certifications, # â† A la BD
        "languages": languages            # â† A la BD
    }
```

---

## ğŸ”€ Independencia vs Acoplamiento

### Estado Actual

| Componente | Usa text_vectorization_service | Usa nlp_service | Usa spaCy | Tipo |
|---|:---:|:---:|:---:|---|
| `_extract_resume_analysis()` | âœ… **SÃ** | âŒ No | âŒ No | Semi-dependiente |
| `_extract_harvard_cv_fields()` | âŒ No | âŒ No | âŒ No | Independiente |
| `upload_resume()` | âœ… (indirecto) | âŒ No | âŒ No | Semi-dependiente |
| `text_vectorization_service` | N/A | âŒ No | âŒ No | Puro (TF-IDF) |
| `nlp_service` (legacy) | âŒ No | N/A | âŒ No | Puro (TF-IDF) |

### Diagrama de Dependencias

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARQUITECTURA ACTUAL                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  upload_resume()                                                 â”‚
â”‚  â”œâ”€ _extract_resume_analysis()                                  â”‚
â”‚  â”‚  â””â”€ text_vectorization_service.analyze_document()            â”‚
â”‚  â”‚     â”œâ”€ normalize_text()                                      â”‚
â”‚  â”‚     â”œâ”€ TermExtractor.extract_technical_terms()              â”‚
â”‚  â”‚     â”œâ”€ TermExtractor.extract_soft_skills()                  â”‚
â”‚  â”‚     â””â”€ TermExtractor.extract_keyphrases()                   â”‚
â”‚  â”‚                                                               â”‚
â”‚  â”œâ”€ _extract_harvard_cv_fields()  â—€â”€â”€â”€ INDEPENDIENTE            â”‚
â”‚  â”‚  â”œâ”€ re.search() para educaciÃ³n                              â”‚
â”‚  â”‚  â”œâ”€ re.search() para experiencia                            â”‚
â”‚  â”‚  â”œâ”€ re.search() para certificaciones                        â”‚
â”‚  â”‚  â””â”€ re.search() para idiomas                                â”‚
â”‚  â”‚                                                               â”‚
â”‚  â””â”€ Guardar en BD + Retornar response                           â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Oportunidades con spaCy

### Problema Actual (Sin spaCy)

```python
# EXTRACCIÃ“N BASADA EN KEYWORDS (FrÃ¡gil)

# Ejemplo 1: Reconocimiento de entidades
Text: "TrabajÃ© como Senior Developer en Google por 3 aÃ±os"
Current regex-based:
  - Busca "Senior" â†’ Encuentra posiciÃ³n
  - Busca "Google" â†’ Encuentra empresa (por keyword matching)
  - Busca "3 aÃ±os" â†’ Encuentra fecha
  - âŒ Problema: Si dice "Fui responsable de...", no detecta la posiciÃ³n

# Ejemplo 2: ExtracciÃ³n de educaciÃ³n
Text: "Licenciatura en IngenierÃ­a en Sistemas por Universidad Nacional"
Current regex-based:
  - Busca "educaciÃ³n|education" en header
  - âŒ Problema: Si estÃ¡ en otra secciÃ³n o sin header, no se detecta
  - âŒ Problema: Si dice "CursÃ© IngenierÃ­a..." sin "Licenciatura", no funciona
```

### Oportunidades con spaCy

```python
# EXTRACCIÃ“N BASADA EN NER + LINGÃœÃSTICA (Robusto)

import spacy
from spacy import displacy

nlp = spacy.load("es_core_news_sm")

text = "TrabajÃ© como Senior Developer en Google por 3 aÃ±os desde 2020 a 2023"
doc = nlp(text)

# 1ï¸âƒ£ Named Entity Recognition (NER)
for ent in doc.ents:
    print(f"Entity: {ent.text:20} | Label: {ent.label_:10} | Span: {ent.start_char}-{ent.end_char}")

# OUTPUT:
# Entity: Senior Developer   | Label: MISC           (OcupaciÃ³n)
# Entity: Google             | Label: ORG            (OrganizaciÃ³n)
# Entity: 3 aÃ±os             | Label: DATE           (DuraciÃ³n)
# Entity: 2020               | Label: DATE           (AÃ±o inicio)
# Entity: 2023               | Label: DATE           (AÃ±o fin)

# 2ï¸âƒ£ Dependency Parsing
for token in doc:
    print(f"Token: {token.text:15} | POS: {token.pos_:10} | DEP: {token.dep_:10} | HEAD: {token.head.text}")

# OUTPUT:
# Token: TrabajÃ©         | POS: VERB      | DEP: ROOT
# Token: como            | POS: ADP       | DEP: case        | HEAD: Developer
# Token: Senior          | POS: ADJ       | DEP: amod        | HEAD: Developer
# Token: Developer       | POS: NOUN      | DEP: obl         | HEAD: TrabajÃ©
# Token: en              | POS: ADP       | DEP: case        | HEAD: Google
# Token: Google          | POS: PROPN     | DEP: obl         | HEAD: TrabajÃ©
# ...

# 3ï¸âƒ£ POS Tagging (Part of Speech)
pos_tags = [(token.text, token.pos_) for token in doc]
# POS helps identify:
# - VERB: acciones (trabajÃ©, desarrollÃ©, implementÃ©)
# - NOUN: conceptos (proyecto, aplicaciÃ³n, sistema)
# - ADJ: calificadores (senior, junior, grande)
# - PROPN: nombres propios (Google, Python, React)

# 4ï¸âƒ£ Lemmatization
lemmas = [(token.text, token.lemma_) for token in doc]
# "trabajÃ©" â†’ "trabajar"
# "desarrollado" â†’ "desarrollar"
# "sistemas" â†’ "sistema"
```

### Beneficios EspecÃ­ficos para MoirAI

#### 1ï¸âƒ£ ExtracciÃ³n de EducaciÃ³n (Mejorada)

```python
# ANTES (Regex - FrÃ¡gil)
education_section_match = re.search(
    r'(educaciÃ³n|education|formaciÃ³n)[\s\n]+(.*?)(?:experiencia|experience|...)',
    text_lower, re.DOTALL
)
# âŒ Si no hay header "EducaciÃ³n", no encuentra nada

# DESPUÃ‰S (spaCy + NER - Robusto)
def extract_education_with_spacy(text):
    doc = nlp(text)
    educations = []
    
    # Detecta entidades EDUCACIÃ“N por NER
    for ent in doc.ents:
        if ent.label_ in ["ORG", "PRODUCT"]:  # Universidad/instituciÃ³n
            # Busca en contexto si hay grado acadÃ©mico nearby
            context_tokens = [t.text for t in ent.sent]
            if any(kw in context_tokens for kw in ["licenciatura", "bachelor", "maestrÃ­a", "master"]):
                educations.append({
                    "institution": ent.text,
                    "sentence_context": ent.sent.text  # Para post-procesamiento
                })
    
    return educations

# RESULTADO: Detecta "Universidad Nacional" aunque estÃ© en pÃ¡rrafo sin header
```

#### 2ï¸âƒ£ ExtracciÃ³n de Experiencia (Mejorada)

```python
# ANTES (Regex - Solo aÃ±os)
dates_match = re.search(r'(20\d{2})[/-]?(20\d{2})?', line)

# DESPUÃ‰S (spaCy + Temporal reasoning)
def extract_experience_with_spacy(text):
    doc = nlp(text)
    experiences = []
    
    for sent in doc.sents:
        # Busca verbos de acciÃ³n (trabajar, desarrollar, implementar)
        action_verbs = ["trabajar", "desarrollar", "implementar", "crear", "gestionar"]
        
        has_action = any(t.lemma_ in action_verbs for t in sent)
        if not has_action:
            continue
        
        # Extrae entidades relevantes
        org = None
        dates = []
        job_title = None
        
        for ent in sent.ents:
            if ent.label_ == "ORG":
                org = ent.text
            elif ent.label_ == "DATE":
                dates.append(ent.text)
            # Detecta posiciÃ³n por POS (NOUN despuÃ©s de preposiciÃ³n "como")
            
        # Construye experiencia
        if org or has_action:
            experiences.append({
                "description": sent.text,
                "company": org,
                "dates": dates,
                "sentence_context": sent
            })
    
    return experiences

# RESULTADO: Detecta "ImplementÃ© un sistema de recomendaciones en Amazon"
# aunque estÃ© redactado de forma distinta
```

#### 3ï¸âƒ£ DetecciÃ³n de Soft Skills Inferidas (Nuevo)

```python
# ANTES (Basado solo en SOFT_SKILLS_VOCAB)
def extract_soft_skills(text):
    for skill in SOFT_SKILLS_VOCAB:
        if skill in text.lower():
            yield skill

# âŒ Si dice "TrabajÃ© bajo presiÃ³n en equipo multidisciplinario",
#   NO detecta "adaptabilidad" ni "trabajo en equipo"

# DESPUÃ‰S (spaCy + AnÃ¡lisis contextual)
def extract_soft_skills_inferred_with_spacy(text):
    doc = nlp(text)
    inferred_skills = []
    
    # Mapeo de contextos â†’ soft skills
    skill_inferences = {
        "bajo presiÃ³n": "adaptabilidad",
        "equipo multidisciplinario": "trabajo en equipo",
        "liderÃ© el proyecto": "liderazgo",
        "resolvÃ­ problemas": "problem solving",
        "presentÃ© al cliente": "comunicaciÃ³n",
        "aprendÃ­ rÃ¡pidamente": "aprendizaje continuo",
    }
    
    for phrase, skill in skill_inferences.items():
        if phrase in text.lower():
            # Localiza la menciÃ³n en el documento
            found_sent = None
            for sent in doc.sents:
                if phrase in sent.text.lower():
                    found_sent = sent
                    break
            
            inferred_skills.append({
                "skill": skill,
                "type": "inferred",
                "source_phrase": phrase,
                "confidence": 0.85,
                "context": found_sent.text if found_sent else text
            })
    
    return inferred_skills

# RESULTADO: Detecta skills INFERIDAS que el CV no menciona explÃ­citamente
```

---

## ğŸ—ï¸ Arquitectura Propuesta

### Fase 1: IntegraciÃ³n BÃ¡sica de spaCy (Semana 1-2)

```
ANTES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  _extract_resume_analysis()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â†’ text_vectorization_service
             â”œâ”€ TermExtractor (keyword-based)
             â””â”€ [Regex para Harvard CV fields]  âŒ FrÃ¡gil

DESPUÃ‰S (FASE 1):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  _extract_resume_analysis()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â†’ text_vectorization_service âœ… (sin cambios)
         â”‚
         â””â”€â†’ spacy_nlp_service (NEW)  âœ… (alternativa)
             â”œâ”€ SpacyEntityExtractor
             â”‚  â””â”€ extract_education_entities()
             â”‚  â””â”€ extract_experience_entities()
             â”‚  â””â”€ extract_organizations()
             â”œâ”€ SpacyInferenceEngine
             â”‚  â””â”€ infer_soft_skills()
             â”‚  â””â”€ infer_seniority_level()
             â””â”€ SpacyDependencyParser
                â””â”€ extract_relationships()

HARVARD CV FIELDS (Mejorado):
â”œâ”€ _extract_harvard_cv_fields() [KEEP REGEX]
â”œâ”€ + _extract_harvard_cv_fields_spacy() [NEW]
â””â”€ Usa mejor score (spaCy si disponible, fallback a regex)
```

### Fase 2: FusiÃ³n Inteligente (Semana 3-4)

```python
# Nueva funciÃ³n orquestadora
def _extract_resume_unified(resume_text):
    """
    Orquesta mÃºltiples engines NLP:
    1. text_vectorization_service (TF-IDF + Keywords)
    2. spacy_nlp_service (NER + Dependency + Inference)
    3. Selecciona el mejor resultado basado en confidence
    """
    
    # Engine 1: Text Vectorization (Fast, Lightweight)
    tfidf_results = _extract_resume_analysis(resume_text)
    
    # Engine 2: spaCy (Slow but Accurate)
    spacy_results = _extract_resume_analysis_spacy(resume_text)
    
    # Merge con confidence scoring
    merged = {
        "skills": merge_results(
            tfidf_results["skills"],
            spacy_results["entities"]["skills"],
            weight_spacy=0.7  # Confiar mÃ¡s en spaCy
        ),
        "soft_skills": merge_results(
            tfidf_results["soft_skills"],
            spacy_results["inferred_soft_skills"],
            weight_spacy=0.9  # Confiar MÃS en spaCy (inferencias mejores)
        ),
        "projects": merge_results(
            tfidf_results["projects"],
            spacy_results["entities"]["projects"],
            weight_spacy=0.6
        ),
        "confidence": max(
            tfidf_results["confidence"],
            spacy_results["overall_confidence"]
        ),
        "extraction_method": "unified"  # Indica que usÃ³ ambos engines
    }
    
    return merged
```

### Fase 3: AsincronÃ­a y Cache (Semana 5)

```python
# Procesamiento en background (no bloquea el upload)
@router.post("/upload_resume")
async def upload_resume(...):
    # AnÃ¡lisis rÃ¡pido (Vectorization) - sÃ­ncrono
    fast_results = _extract_resume_analysis(resume_text)
    
    # AnÃ¡lisis profundo (spaCy) - background
    background_tasks.add_task(
        _analyze_with_spacy_async,
        student_id=student.id,
        resume_text=resume_text
    )
    
    # Retorna resultados rÃ¡pidos inmediatamente
    return ResumeAnalysisResponse(
        student=student_profile,
        extracted_skills=fast_results["skills"],
        extraction_method="fast",
        note="AnÃ¡lisis profundo en progreso..."
    )

# Actualiza con resultados mejores cuando termine
async def _analyze_with_spacy_async(student_id, resume_text):
    spacy_results = _extract_resume_analysis_spacy(resume_text)
    
    # Actualiza en BD con resultado mejorado
    student = await session.get(Student, student_id)
    student.soft_skills_inferred = json.dumps(spacy_results["inferred_soft_skills"])
    student.spacy_analysis_confidence = spacy_results["overall_confidence"]
    await session.commit()
    
    # WebSocket notification (opcional)
    await notify_student_analysis_complete(student_id, spacy_results)
```

---

## ğŸ›£ï¸ Roadmap de ImplementaciÃ³n

### Semana 1: Setup spaCy

```bash
# 1. Instalar spaCy y modelos
pip install spacy==3.7.2
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm

# 2. Crear app/services/spacy_nlp_service.py (~300 lÃ­neas)
touch app/services/spacy_nlp_service.py

# 3. Tests iniciales
python -m pytest tests/unit/test_spacy_extraction.py -v
```

**Estructura de spacy_nlp_service.py**:
```python
"""
spaCy-based NLP Service para MoirAI
Specializado en Named Entity Recognition y Dependency Parsing
"""

import spacy
from typing import List, Dict, Optional
from app.core.config import settings

class SpacyNLPService:
    def __init__(self):
        self.nlp_es = spacy.load("es_core_news_sm")
        self.nlp_en = spacy.load("en_core_web_sm")
    
    def extract_entities(self, text: str, lang: str = "es"):
        """Extrae entidades nombradas"""
        nlp = self.nlp_es if lang == "es" else self.nlp_en
        doc = nlp(text)
        return [(ent.text, ent.label_) for ent in doc.ents]
    
    def extract_education_entities(self, text: str):
        """Extrae educaciÃ³n usando spaCy NER + heurÃ­sticas"""
        # Implementation
        pass
    
    def extract_experience_entities(self, text: str):
        """Extrae experiencia laboral"""
        # Implementation
        pass
    
    def infer_soft_skills(self, text: str):
        """Infiere soft skills del contexto"""
        # Implementation
        pass

spacy_nlp_service = SpacyNLPService()
```

### Semana 2: IntegraciÃ³n en _extract_resume_analysis

```python
# Modificar students.py para usar spaCy como alternativa

def _extract_resume_analysis_spacy(resume_text: str) -> dict:
    """Nueva versiÃ³n con spaCy"""
    try:
        results = spacy_nlp_service.extract_entities(resume_text)
        # Procesar resultados...
        return {"skills": [], "soft_skills": [], ...}
    except Exception as e:
        # Fallback a versiÃ³n anterior
        return _extract_resume_analysis(resume_text)
```

### Semana 3-4: Merge y Unified Engine

```python
def _extract_resume_unified(resume_text: str) -> dict:
    """Combina TF-IDF + spaCy con confidence weighting"""
    # Implementation
    pass
```

### Semana 5: Async Processing + Cache

```python
@router.post("/upload_resume")
async def upload_resume(...):
    # AnÃ¡lisis rÃ¡pido sincrono
    # AnÃ¡lisis profundo en background
    # Cache en Redis para futuros lookups
```

---

## ğŸ“Š Comparativa: MÃ©todos de ExtracciÃ³n

| Aspecto | Regex (Actual) | TF-IDF (text_vectorization) | spaCy (Propuesto) |
|---|---|---|---|
| **Velocidad** | âš¡ Muy rÃ¡pido (1ms) | âš¡ RÃ¡pido (5ms) | ğŸ¢ Lento (50-100ms) |
| **PrecisiÃ³n EducaciÃ³n** | 60% | 65% | **95%** |
| **PrecisiÃ³n Experiencia** | 65% | 70% | **92%** |
| **Soft Skills Inferidas** | âŒ No | âŒ No | âœ… **SÃ­** |
| **Depende de Keywords** | âœ… SÃ­ | âœ… SÃ­ | âŒ **No** |
| **Maneja Variaciones** | âŒ No | âœ… Parcial | âœ… **SÃ­** |
| **Multiidioma** | âŒ No | âœ… Parcial | âœ… **SÃ­** |
| **Memoria Requerida** | 1MB | 2MB | **200MB** |
| **Recomendado Para** | MVP RÃ¡pido | ProducciÃ³n Ligera | ProducciÃ³n Robusta |

---

## ğŸ¯ RecomendaciÃ³n

### Para MoirAI AHORA (MVP):

```
âœ… MANTENER: text_vectorization_service
   - Suficiente para extracciÃ³n bÃ¡sica
   - RÃ¡pido y eficiente
   - Sin dependencias pesadas

â³ PREPARAR: spacy_nlp_service
   - Como alternativa opcional
   - Para anÃ¡lisis mÃ¡s profundos
   - En background tasks

ğŸ”„ INTEGRAR: Fusion engine (Fase 2)
   - Usa TF-IDF para resultado rÃ¡pido
   - Usa spaCy en background
   - Actualiza BD cuando mejora disponible
```

### DecisiÃ³n de Arquitectura

```python
# En settings (configurable):
NLP_FAST_MODE = True  # Usar text_vectorization por defecto

if NLP_FAST_MODE:
    results = _extract_resume_analysis(resume_text)  # 5ms
    # En background:
    background_tasks.add_task(_extract_resume_analysis_spacy, ...)
else:
    results = _extract_resume_analysis_spacy(resume_text)  # 100ms
```

---

## ğŸ“š Referencias

- spaCy Documentation: https://spacy.io/
- spaCy Models (Spanish): https://spacy.io/models/es
- Named Entity Recognition: https://es.wikipedia.org/wiki/Reconocimiento_de_entidades_nombradas
- Dependency Parsing: https://spacy.io/usage/linguistic-features#dependency-parse
