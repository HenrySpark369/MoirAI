#!/usr/bin/env python3
"""
NLP Analysis Service - Evaluaci√≥n de Dataset de CVs Sint√©ticos
Prueba m√©tricas de calidad: F1-score, accuracy, TF-IDF, BoW, LDA
"""

import sqlite3
import json
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.decomposition import LatentDirichletAllocation
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import spacy
from spacy.lang.es import Spanish
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n
DB_PATH = 'cv_simulator/training_data_cvs.db'
STOPWORDS_ES = set(stopwords.words('spanish')) if 'spanish' in stopwords.fileids() else set()

# Descargar recursos NLTK si no existen
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)

class NLPAnalyzer:
    """Analizador NLP para evaluar calidad de dataset de CVs"""

    def __init__(self, db_path):
        self.db_path = db_path
        self.nlp = Spanish()
        # Agregar sentencizer para an√°lisis de oraciones
        if 'sentencizer' not in self.nlp.pipe_names:
            self.nlp.add_pipe('sentencizer')
        self.data = []
        self.load_data()

    def load_data(self):
        """Cargar datos desde SQLite"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT industry, seniority, cv_text, annotations FROM cv_dataset")
        rows = cursor.fetchall()

        for row in rows:
            industry, seniority, cv_text, annotations_json = row
            try:
                annotations = json.loads(annotations_json)
                self.data.append({
                    'industry': industry,
                    'seniority': seniority,
                    'cv_text': cv_text,
                    'annotations': annotations
                })
            except json.JSONDecodeError:
                continue

        conn.close()
        print(f"‚úÖ Cargados {len(self.data)} CVs desde la base de datos")

    def analyze_basic_stats(self):
        """Estad√≠sticas b√°sicas del dataset"""
        print("\n" + "="*60)
        print("üìä ESTAD√çSTICAS B√ÅSICAS DEL DATASET")
        print("="*60)

        df = pd.DataFrame(self.data)

        print(f"Total de CVs: {len(df)}")

        # Distribuci√≥n por industria
        print("\nüìà Distribuci√≥n por Industria:")
        industry_counts = df['industry'].value_counts()
        for industry, count in industry_counts.items():
            pct = count / len(df) * 100
            print(f"  {industry}: {count} ({pct:.1f}%)")

        # Distribuci√≥n por seniority
        print("\nüìà Distribuci√≥n por Seniority:")
        seniority_counts = df['seniority'].value_counts()
        for seniority, count in seniority_counts.items():
            pct = count / len(df) * 100
            print(f"  {seniority}: {count} ({pct:.1f}%)")

        # Estad√≠sticas de texto
        text_lengths = df['cv_text'].str.len()
        print("\nüìè Estad√≠sticas de Longitud de Texto:")
        print(f"  Promedio: {text_lengths.mean():.0f} caracteres")
        print(f"  M√≠nimo: {text_lengths.min()} caracteres")
        print(f"  M√°ximo: {text_lengths.max()} caracteres")

        # Skills m√°s comunes
        all_skills = []
        for item in self.data:
            skills = item['annotations'].get('skills', [])
            all_skills.extend(skills)

        skill_counts = Counter(all_skills)
        print("\nüîß Top 10 Skills m√°s comunes:")
        for skill, count in skill_counts.most_common(10):
            print(f"  {skill}: {count}")

    def analyze_text_quality(self):
        """An√°lisis de calidad del texto usando spaCy"""
        print("\n" + "="*60)
        print("üîç AN√ÅLISIS DE CALIDAD DE TEXTO (spaCy)")
        print("="*60)

        total_sentences = 0
        total_tokens = 0
        pos_counts = Counter()

        for item in self.data[:10]:  # Analizar solo primeros 10 para demo
            doc = self.nlp(item['cv_text'])
            total_sentences += len(list(doc.sents))
            total_tokens += len(doc)

            for token in doc:
                pos_counts[token.pos_] += 1

        print(f"Promedio de oraciones por CV: {total_sentences/10:.1f}")
        print(f"Promedio de tokens por CV: {total_tokens/10:.1f}")

        print("\nüè∑Ô∏è  Distribuci√≥n de Partes del Discurso (Top 5):")
        for pos, count in pos_counts.most_common(5):
            print(f"  {pos}: {count}")

    def tfidf_analysis(self):
        """An√°lisis TF-IDF por industria"""
        print("\n" + "="*60)
        print("üìä AN√ÅLISIS TF-IDF POR INDUSTRIA")
        print("="*60)

        df = pd.DataFrame(self.data)

        # Preparar textos limpios
        texts = []
        for text in df['cv_text']:
            # Limpieza b√°sica
            text = text.lower()
            tokens = word_tokenize(text, language='spanish')
            tokens = [t for t in tokens if t.isalnum() and t not in STOPWORDS_ES and len(t) > 2]
            texts.append(' '.join(tokens))

        # TF-IDF
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform(texts)

        # Palabras m√°s importantes por industria
        feature_names = vectorizer.get_feature_names_out()

        for industry in df['industry'].unique():
            industry_texts = df[df['industry'] == industry]['cv_text']
            if len(industry_texts) < 3:
                continue

            industry_clean = []
            for text in industry_texts:
                text = text.lower()
                tokens = word_tokenize(text, language='spanish')
                tokens = [t for t in tokens if t.isalnum() and t not in STOPWORDS_ES and len(t) > 2]
                industry_clean.append(' '.join(tokens))

            industry_tfidf = vectorizer.transform(industry_clean)
            avg_tfidf = np.asarray(industry_tfidf.mean(axis=0)).flatten()

            top_indices = avg_tfidf.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_indices]

            print(f"\nüè≠ {industry} (Top 10 palabras TF-IDF):")
            for word in top_words:
                print(f"  {word}")

    def bow_analysis(self):
        """An√°lisis Bag of Words"""
        print("\n" + "="*60)
        print("üì¶ AN√ÅLISIS BAG OF WORDS (BoW)")
        print("="*60)

        df = pd.DataFrame(self.data)

        # BoW simple
        vectorizer = CountVectorizer(max_features=500, stop_words=list(STOPWORDS_ES))
        bow_matrix = vectorizer.fit_transform(df['cv_text'])

        print(f"Vocabulario total: {len(vectorizer.get_feature_names_out())} palabras")
        print(f"Matriz BoW: {bow_matrix.shape[0]} documentos x {bow_matrix.shape[1]} features")

        # Palabras m√°s frecuentes globalmente
        word_freq = bow_matrix.sum(axis=0).A1
        word_names = vectorizer.get_feature_names_out()

        top_indices = word_freq.argsort()[-20:][::-1]
        print("\nüìà Top 20 palabras m√°s frecuentes:")
        for i, idx in enumerate(top_indices):
            print(f"  {i+1:2d}. {word_names[idx]:15s}: {word_freq[idx]}")

    def lda_topic_modeling(self):
        """An√°lisis tem√°tico supervisado basado en secciones de CV con preprocesamiento avanzado"""
        print("\n" + "="*60)
        print("üé≠ AN√ÅLISIS TEM√ÅTICO SUPERVISADO - SECCIONES DE CV")
        print("="*60)

        # Definir temas principales de CV (supervisados)
        cv_themes = {
            'educaci√≥n': {
                'keywords': ['educaci√≥n', 'estudios', 'universidad', 'carrera', 'licenciatura', 'maestr√≠a', 'doctorado', 'bachillerato', 'diploma', 'certificaci√≥n', 't√≠tulo', 'acad√©mico', 'graduado', 'egresado'],
                'description': 'Formaci√≥n acad√©mica y t√≠tulos obtenidos'
            },
            'experiencia_profesional': {
                'keywords': ['experiencia', 'profesional', 'trabajo', 'empleo', 'cargo', 'puesto', 'rol', 'posici√≥n', 'empresa', 'compa√±√≠a', 'organizaci√≥n', 'responsabilidades', 'logros', 'proyectos'],
                'description': 'Historial laboral y responsabilidades'
            },
            'habilidades_duras': {
                'keywords': ['habilidades', 'competencias', 'tecnolog√≠as', 'herramientas', 'software', 'programaci√≥n', 'python', 'java', 'sql', 'excel', 'power', 'bi', 'tableau', 'aws', 'docker', 'kubernetes', 'machine learning', 'deep learning', 'estad√≠stica', 'matem√°ticas'],
                'description': 'Habilidades t√©cnicas y herramientas espec√≠ficas'
            },
            'habilidades_blandas': {
                'keywords': ['liderazgo', 'comunicaci√≥n', 'trabajo en equipo', 'adaptabilidad', 'resoluci√≥n de problemas', 'creatividad', 'gesti√≥n del tiempo', 'aprendizaje continuo', 'empat√≠a', 'colaboraci√≥n', 'motivaci√≥n', 'iniciativa', 'flexibilidad', 'pensamiento cr√≠tico'],
                'description': 'Competencias interpersonales y comportamentales'
            },
            'idiomas': {
                'keywords': ['idiomas', 'espa√±ol', 'ingl√©s', 'franc√©s', 'alem√°n', 'italiano', 'portugu√©s', 'chino', 'japon√©s', 'nativo', 'avanzado', 'intermedio', 'b√°sico', 'fluido', 'conversacional'],
                'description': 'Dominio de idiomas y nivel de competencia'
            },
            'contacto_informaci√≥n': {
                'keywords': ['tel√©fono', 'email', 'correo', 'electr√≥nico', 'direcci√≥n', 'ubicaci√≥n', 'ciudad', 'estado', 'pa√≠s', 'linkedin', 'github', 'portfolio', 'sitio web', 'contacto'],
                'description': 'Informaci√≥n de contacto y presencia digital'
            }
        }

        df = pd.DataFrame(self.data)

        # An√°lisis de cobertura por tema
        theme_coverage = {theme: [] for theme in cv_themes.keys()}

        for idx, row in df.iterrows():
            cv_text = row['cv_text'].lower()

            for theme, config in cv_themes.items():
                # Contar ocurrencias de keywords del tema
                keyword_count = sum(cv_text.count(keyword) for keyword in config['keywords'])
                # Normalizar por longitud del texto
                coverage_score = min(keyword_count / max(len(cv_text.split()), 1) * 100, 100)
                theme_coverage[theme].append(coverage_score)

        # Estad√≠sticas de cobertura
        print("üìä COBERTURA PROMEDIO POR TEMA EN CVs:")
        print("-" * 50)

        for theme, scores in theme_coverage.items():
            avg_coverage = np.mean(scores)
            max_coverage = np.max(scores)
            min_coverage = np.min(scores)
            coverage_pct = np.mean([1 if score > 0 else 0 for score in scores]) * 100

            print(f"üéØ {theme.replace('_', ' ').title()}:")
            print(f"   üìà Cobertura promedio: {avg_coverage:.2f}%")
            print(f"   üéØ CVs que lo incluyen: {coverage_pct:.1f}%")
            print(f"   üìä Rango: {min_coverage:.2f}% - {max_coverage:.2f}%")
            print(f"   üìù {cv_themes[theme]['description']}")
            print()

        # An√°lisis LDA mejorado con preprocesamiento avanzado
        print("üîç AN√ÅLISIS LDA MEJORADO (5 temas autom√°ticos - sin contaminaci√≥n de contacto):")
        print("-" * 50)

        # Funci√≥n de preprocesamiento avanzado
        def preprocess_cv_text(text):
            """Preprocesamiento avanzado para eliminar ruido de contacto y mejorar calidad"""
            import re

            # Convertir a min√∫sculas
            text = text.lower()

            # Eliminar informaci√≥n de contacto espec√≠fica
            contact_patterns = [
                r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',  # Tel√©fonos US
                r'\+\d{1,3}[-.\s]?\d{3}[-.\s]?\d{3}[-.\s]?\d{4}',  # Tel√©fonos internacionales
                r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Emails
                r'@\w+',  # Menciones @usuario
                r'linkedin\.com/\S+',  # LinkedIn URLs
                r'github\.com/\S+',  # GitHub URLs
                r'http[s]?://\S+',  # URLs generales
                r'\b\d{5}(?:[-\s]\d{4})?\b',  # C√≥digos postales US
                r'\b\d{4,5}\b',  # C√≥digos postales otros
            ]

            for pattern in contact_patterns:
                text = re.sub(pattern, '', text, flags=re.IGNORECASE)

            # Tokenizaci√≥n y limpieza
            tokens = word_tokenize(text, language='spanish')

            # Filtros adicionales
            custom_stopwords = set(STOPWORDS_ES)
            custom_stopwords.update([
                'tel√©fono', 'email', 'correo', 'electr√≥nico', 'direcci√≥n',
                'ciudad', 'estado', 'pa√≠s', 'ubicaci√≥n', 'contacto',
                'fecha', 'nacimiento', 'edad', 'g√©nero', 'nacionalidad'
            ])

            # Filtrar tokens
            tokens = [
                token for token in tokens
                if (token.isalnum() and
                    len(token) > 2 and
                    token not in custom_stopwords and
                    not token.isdigit())
            ]

            return ' '.join(tokens)

        # Aplicar preprocesamiento a todos los CVs
        processed_texts = [preprocess_cv_text(row['cv_text']) for _, row in df.iterrows()]

        # Vectorizaci√≥n con par√°metros optimizados
        vectorizer = CountVectorizer(
            max_features=1000,
            stop_words=list(STOPWORDS_ES),
            min_df=2,  # Aparece en al menos 2 documentos
            max_df=0.8,  # No m√°s del 80% de documentos
            ngram_range=(1, 2)  # Unigramas y bigramas
        )

        bow_matrix = vectorizer.fit_transform(processed_texts)

        # LDA con par√°metros optimizados
        n_topics = 5
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            learning_method='online',
            max_iter=20,
            learning_decay=0.7,
            evaluate_every=5
        )

        lda.fit(bow_matrix)

        feature_names = vectorizer.get_feature_names_out()

        print(f"Modelo LDA optimizado con {n_topics} temas entrenado")
        print(f"Vocabulario: {len(feature_names)} t√©rminos")
        print(f"Documentos procesados: {len(processed_texts)}")

        # An√°lisis de coherencia de temas
        def calculate_topic_coherence(topic_words, documents, top_n=10):
            """Calcular coherencia de un tema usando PMI"""
            coherence = 0
            word_pairs = 0

            for i in range(len(topic_words)):
                for j in range(i+1, len(topic_words)):
                    word1, word2 = topic_words[i], topic_words[j]

                    # Contar co-ocurrencias en documentos
                    co_occurrences = sum(1 for doc in documents
                                       if word1 in doc and word2 in doc)

                    # Contar ocurrencias individuales
                    word1_count = sum(1 for doc in documents if word1 in doc)
                    word2_count = sum(1 for doc in documents if word2 in doc)

                    if word1_count > 0 and word2_count > 0 and co_occurrences > 0:
                        # PMI simplificado
                        pmi = np.log((co_occurrences * len(documents)) /
                                   (word1_count * word2_count))
                        coherence += pmi
                        word_pairs += 1

            return coherence / word_pairs if word_pairs > 0 else 0

        # Mostrar temas con m√©tricas de coherencia
        topic_coherences = []

        for topic_idx, topic in enumerate(lda.components_):
            top_word_indices = topic.argsort()[:-11:-1]
            top_words = [feature_names[i] for i in top_word_indices]

            # Calcular coherencia
            coherence = calculate_topic_coherence(top_words, processed_texts)
            topic_coherences.append(coherence)

            print(f"\nüéØ Tema {topic_idx + 1} (Coherencia: {coherence:.3f}):")
            print(f"   Palabras clave: {', '.join(top_words)}")

        # Estad√≠sticas de coherencia general
        avg_coherence = np.mean(topic_coherences)
        print(f"\nüìä COHERENCIA GENERAL DE TEMAS:")
        print(f"   Coherencia promedio: {avg_coherence:.3f}")
        print(f"   Mejor tema: Tema {np.argmax(topic_coherences) + 1} ({np.max(topic_coherences):.3f})")
        print(f"   Peor tema: Tema {np.argmin(topic_coherences) + 1} ({np.min(topic_coherences):.3f})")

        # An√°lisis de distribuci√≥n de temas en documentos
        doc_topics = lda.transform(bow_matrix)
        dominant_themes = np.argmax(doc_topics, axis=1)

        print(f"\nüìä DISTRIBUCI√ìN DE TEMAS EN {len(df)} CVs:")
        theme_counts = Counter(dominant_themes)
        for topic_idx in range(n_topics):
            count = theme_counts.get(topic_idx, 0)
            percentage = count / len(df) * 100
            print(f"   Tema {topic_idx + 1}: {count} CVs ({percentage:.1f}%)")

        # Validaci√≥n cruzada: Comparar temas supervisados vs LDA mejorado
        print(f"\n‚úÖ VALIDACI√ìN: TEMAS SUPERVISADOS vs LDA MEJORADO")
        print("-" * 50)
        print("‚úì Preprocesamiento avanzado elimina contaminaci√≥n de contacto")
        print("‚úì Temas supervisados garantizan cobertura de secciones cr√≠ticas")
        print("‚úì LDA mejorado encuentra patrones naturales sin ruido")
        print("‚úì M√©tricas de coherencia validan calidad de separaci√≥n tem√°tica")

        # Recomendaciones basadas en an√°lisis
        print(f"\nüí° RECOMENDACIONES PARA MEJORA DE CVs:")
        low_coverage_themes = []
        for theme, scores in theme_coverage.items():
            coverage_pct = np.mean([1 if score > 0 else 0 for score in scores]) * 100
            if coverage_pct < 50:
                low_coverage_themes.append((theme, coverage_pct))

        if low_coverage_themes:
            print("Secciones con baja cobertura que necesitan mejora:")
            for theme, pct in low_coverage_themes:
                print(f"   ‚Ä¢ {theme.replace('_', ' ').title()}: {pct:.1f}%")
        else:
            print("‚úì Todas las secciones cr√≠ticas tienen buena cobertura")

    def automatic_cv_classification(self):
        """Clasificaci√≥n autom√°tica de CVs usando t√©cnicas del cuaderno"""
        print("\n" + "="*60)
        print("ü§ñ CLASIFICACI√ìN AUTOM√ÅTICA DE CVs")
        print("="*60)

        if len(self.data) < 20:
            print("‚ùå Insuficientes datos para clasificaci√≥n")
            return

        # Preparar datos de entrenamiento
        cv_texts = []
        industries = []
        seniorities = []

        for item in self.data:
            cv_texts.append(item['cv_text'])
            industries.append(item['industry'])
            seniorities.append(item['seniority'])

        # Funci√≥n de preprocesamiento (simplificada del cuaderno)
        def preprocess_cv_text_simple(text):
            """Preprocesamiento simple inspirado en el cuaderno"""
            import re
            import string

            text = str(text).lower()
            text = re.sub(r'\[.*?\]', '', text)
            text = re.sub(r'https?://\S+|www\.\S+', '', text)
            text = re.sub(r'<.*?>+', '', text)
            text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
            text = re.sub(r'\n', ' ', text)
            text = re.sub(r'\w*\d\w*', '', text)
            text = re.sub(r'[^\x00-\x7F]+', '', text)
            text = text.strip()

            # Tokenizaci√≥n y filtrado b√°sico
            tokens = word_tokenize(text, language='spanish')
            tokens = [t for t in tokens if t.isalnum() and t not in STOPWORDS_ES and len(t) > 2]
            return ' '.join(tokens)

        # Preprocesar textos
        processed_texts = [preprocess_cv_text_simple(text) for text in cv_texts]

        # Vectorizaci√≥n TF-IDF (como en el cuaderno)
        tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        X = tfidf.fit_transform(processed_texts)

        # 1. Clasificaci√≥n por Industria
        print("üè≠ CLASIFICACI√ìN POR INDUSTRIA:")
        print("-" * 30)

        X_train_ind, X_test_ind, y_train_ind, y_test_ind = train_test_split(
            X, industries, test_size=0.3, random_state=42, stratify=industries
        )

        nb_industry = MultinomialNB()
        nb_industry.fit(X_train_ind, y_train_ind)
        y_pred_ind = nb_industry.predict(X_test_ind)

        acc_ind = accuracy_score(y_test_ind, y_pred_ind)
        f1_ind = f1_score(y_test_ind, y_pred_ind, average='weighted')

        print(f"Accuracy: {acc_ind:.3f}")
        print(f"F1-Score (weighted): {f1_ind:.3f}")

        # 2. Clasificaci√≥n por Seniority
        print("\nüìä CLASIFICACI√ìN POR SENIORITY:")
        print("-" * 30)

        X_train_sen, X_test_sen, y_train_sen, y_test_sen = train_test_split(
            X, seniorities, test_size=0.3, random_state=42, stratify=seniorities
        )

        nb_seniority = MultinomialNB()
        nb_seniority.fit(X_train_sen, y_train_sen)
        y_pred_sen = nb_seniority.predict(X_test_sen)

        acc_sen = accuracy_score(y_test_sen, y_pred_sen)
        f1_sen = f1_score(y_test_sen, y_pred_sen, average='weighted')

        print(f"Accuracy: {acc_sen:.3f}")
        print(f"F1-Score (weighted): {f1_sen:.3f}")

        # Guardar modelos (como en el cuaderno)
        import joblib
        import os

        models_dir = "cv_simulator/models"
        os.makedirs(models_dir, exist_ok=True)

        joblib.dump(nb_industry, f"{models_dir}/industry_classifier.pkl")
        joblib.dump(nb_seniority, f"{models_dir}/seniority_classifier.pkl")
        joblib.dump(tfidf, f"{models_dir}/tfidf_vectorizer.pkl")

        print(f"\nÔøΩ Modelos guardados en {models_dir}/")

        # Demo de predicci√≥n (como en el cuaderno)
        print("\nüéØ DEMO DE PREDICCI√ìN:")
        print("-" * 30)

        if self.data:
            sample_cv = self.data[0]['cv_text']
            sample_processed = preprocess_cv_text_simple(sample_cv)
            sample_vector = tfidf.transform([sample_processed])

            pred_industry = nb_industry.predict(sample_vector)[0]
            pred_seniority = nb_seniority.predict(sample_vector)[0]

            print(f"CV de ejemplo: {sample_cv[:100]}...")
            print(f"Predicci√≥n - Industria: {pred_industry}")
            print(f"Predicci√≥n - Seniority: {pred_seniority}")
            print(f"Real - Industria: {industries[0]}, Seniority: {seniorities[0]}")

        return {
            'industry_accuracy': acc_ind,
            'seniority_accuracy': acc_sen,
            'industry_f1': f1_ind,
            'seniority_f1': f1_sen
        }

    def advanced_text_preprocessing(self):
        """Preprocesamiento avanzado de texto inspirado en el cuaderno de rese√±as"""
        print("\n" + "="*60)
        print("üßπ PREPROCESAMIENTO AVANZADO DE TEXTO")
        print("="*60)

        import re
        from nltk.corpus import stopwords
        import string

        # Funci√≥n de limpieza avanzada (inspirada en el cuaderno)
        def clean_text_advanced(text):
            """Limpieza avanzada como en el cuaderno de rese√±as"""
            # Convertir a min√∫sculas
            text = str(text).lower()

            # Eliminar textos entre corchetes (ej.: etiquetas)
            text = re.sub(r'\[.*?\]', '', text)

            # Eliminar URLs
            text = re.sub(r'https?://\S+|www\.\S+', '', text)

            # Eliminar etiquetas HTML
            text = re.sub(r'<.*?>+', '', text)

            # Eliminar signos de puntuaci√≥n
            text = re.sub('[%s]' % re.escape(string.punctuation), '', text)

            # Eliminar saltos de l√≠nea
            text = re.sub(r'\n', ' ', text)

            # Eliminar palabras que contienen n√∫meros
            text = re.sub(r'\w*\d\w*', '', text)

            # Eliminar emojis y caracteres especiales (no ASCII)
            text = re.sub(r'[^\x00-\x7F]+', '', text)

            # Eliminar espacios extras
            text = text.strip()

            return text

        # Funci√≥n de lematizaci√≥n con spaCy
        def lemmatize_with_spacy(text):
            """Lematizaci√≥n usando spaCy como en el cuaderno"""
            doc = self.nlp(text)
            # Eliminar stopwords y aplicar lematizaci√≥n
            lemmatized = [token.lemma_ for token in doc if token.text.lower() not in STOPWORDS_ES]
            return " ".join(lemmatized).strip()

        # Aplicar preprocesamiento a una muestra
        sample_size = min(10, len(self.data))
        print(f"üìä Procesando muestra de {sample_size} CVs...")

        processed_texts = []
        for item in self.data[:sample_size]:
            # Limpieza b√°sica
            clean_text = clean_text_advanced(item['cv_text'])
            # Lematizaci√≥n avanzada
            lemmatized_text = lemmatize_with_spacy(clean_text)
            processed_texts.append(lemmatized_text)

        print("‚úÖ Preprocesamiento completado")
        print(f"ÔøΩ Ejemplo de texto procesado:")
        print(f"   Original: {self.data[0]['cv_text'][:100]}...")
        print(f"   Procesado: {processed_texts[0][:100]}...")

        return processed_texts

    def run_full_analysis(self):
        """Ejecutar an√°lisis completo"""
        print("üöÄ INICIANDO AN√ÅLISIS NLP COMPLETO DEL DATASET")
        print("="*60)

        self.analyze_basic_stats()
        self.analyze_text_quality()
        self.tfidf_analysis()
        self.bow_analysis()
        self.lda_topic_modeling()
        self.classification_experiment()
        self.harvard_style_analysis()

        print("\n" + "="*60)
        print("‚úÖ AN√ÅLISIS COMPLETADO")
        print("="*60)

def main():
    analyzer = NLPAnalyzer(DB_PATH)
    analyzer.run_full_analysis()

if __name__ == "__main__":
    main()
