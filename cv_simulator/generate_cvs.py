import openai
import json
import sqlite3
import time
import uuid
import os
import sys

# Configuraci√≥n para LM Studio Local
API_BASE_URL = "http://127.0.0.1:1234/v1"
API_KEY = "lm-studio" # Clave dummy

client = openai.OpenAI(
    base_url=API_BASE_URL,
    api_key=API_KEY
)

def get_active_model():
    """Obtiene el modelo cargado actualmente en LM Studio"""
    try:
        models = client.models.list()
        if models.data:
            # Retorna el primer modelo activo (LM Studio suele tener uno cargado)
            model_id = models.data[0].id
            print(f"üîå Modelo detectado: {model_id}")
            return model_id
        else:
            print("‚ö†Ô∏è No se detectaron modelos cargados en LM Studio.")
            return "local-model" # Fallback gen√©rico
    except Exception as e:
        print(f"‚ö†Ô∏è Error conectando a LM Studio: {e}")
        return "local-model"

# Detectar modelo al inicio
MODEL_ID = get_active_model()

# 1. Preparar Base de Datos
DB_PATH = 'cv_simulator/training_data_cvs.db'
# Asegurar que el directorio existe si se corre desde root
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS cv_dataset (
        id TEXT PRIMARY KEY,
        industry TEXT,
        seniority TEXT,
        cv_text TEXT,
        annotations JSON,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
''')
conn.commit()

# Configuraci√≥n de Distribuci√≥n Uniforme
TARGET_DISTRIBUTION = {
    'industries': {
        'Tecnolog√≠a': 0.12,
        'Ciencia de Datos': 0.15,
        'Finanzas': 0.12,
        'Salud': 0.12,
        'Biotecnolog√≠a': 0.10,
        'FinTech': 0.08,
        'Healthcare': 0.08,
        'Marketing': 0.08,
        'Legal': 0.07,
        'Construcci√≥n': 0.06,
        'Educaci√≥n': 0.06,
        'Retail': 0.05,
        'Manufactura': 0.01
    },
    'seniorities': {
        'Junior': 0.20,
        'Mid-Level': 0.35,
        'Senior': 0.30,
        'Lead': 0.10,
        'Manager': 0.03,
        'Director': 0.02
    },
    'genders': {
        'Masculino': 0.50,
        'Femenino': 0.50
    },
    'universities': {
        'UNAM': 0.23,
        'IPN': 0.18,
        'UAM': 0.15,
        'UNRC': 0.14,
        'UACM': 0.07,
        'Otros': 0.16
    }
}

class DistributionTracker:
    """Clase para rastrear y balancear la distribuci√≥n de categor√≠as"""

    def __init__(self, target_dist):
        self.target = target_dist
        self.current = {cat: {subcat: 0 for subcat in subcats.keys()}
                       for cat, subcats in target_dist.items()}
        self.total_per_category = {cat: 0 for cat in target_dist.keys()}

    def update(self, category, subcategory):
        """Actualizar contador para una categor√≠a"""
        if category in self.current and subcategory in self.current[category]:
            self.current[category][subcategory] += 1
            self.total_per_category[category] += 1

    def get_distribution_weights(self, category):
        """Calcular pesos para favorecer categor√≠as subrepresentadas"""
        if category not in self.current:
            return {}

        total = self.total_per_category[category]
        if total == 0:
            return {subcat: 1.0 for subcat in self.current[category].keys()}

        weights = {}
        for subcat in self.current[category]:
            current_ratio = self.current[category][subcat] / total
            target_ratio = self.target[category].get(subcat, 0.1)

            # Calcular peso: favorecer subrepresentadas
            if current_ratio < target_ratio:
                # Subrepresentada: aumentar peso
                deficit = target_ratio - current_ratio
                weights[subcat] = 1.0 + (deficit * 2.0)  # Multiplicador
            else:
                # Sobre-representada: reducir peso
                excess = current_ratio - target_ratio
                weights[subcat] = max(0.1, 1.0 - (excess * 1.5))  # M√≠nimo 0.1

        return weights

    def get_weighted_choice(self, category):
        """Seleccionar categor√≠a basada en pesos"""
        weights = self.get_distribution_weights(category)
        if not weights:
            return list(self.current[category].keys())[0]

        # Normalizar pesos
        total_weight = sum(weights.values())
        normalized_weights = {k: v/total_weight for k, v in weights.items()}

        # Selecci√≥n ponderada
        import random
        rand = random.random()
        cumulative = 0.0

        for subcat, weight in normalized_weights.items():
            cumulative += weight
            if rand <= cumulative:
                return subcat

        return list(weights.keys())[-1]  # Fallback

    def get_stats(self):
        """Obtener estad√≠sticas actuales de distribuci√≥n"""
        stats = {}
        for category in self.current:
            total = self.total_per_category[category]
            if total > 0:
                stats[category] = {
                    subcat: {
                        'count': count,
                        'percentage': (count / total) * 100,
                        'target': self.target[category].get(subcat, 0) * 100
                    }
                    for subcat, count in self.current[category].items()
                }
        return stats

    def print_balance_report(self):
        """Imprimir reporte de balance de distribuci√≥n"""
        print("\nüìä REPORTE DE BALANCE DE DISTRIBUCI√ìN:")
        print("=" * 50)

        for category, subcats in self.get_stats().items():
            print(f"\nüîπ {category.upper()}:")
            sorted_subcats = sorted(subcats.items(),
                                  key=lambda x: x[1]['count'],
                                  reverse=True)

            for subcat, data in sorted_subcats:
                status = "‚úÖ" if abs(data['percentage'] - data['target']) < 5 else "‚ö†Ô∏è"
                print(f"  {status} {subcat}: {data['count']} ({data['percentage']:.1f}% | Target: {data['target']:.1f}%)")

# Instancia global del tracker
distribution_tracker = DistributionTracker(TARGET_DISTRIBUTION)

# Configuraci√≥n global
OBJETIVO = 100

def generate_dynamic_prompt():
    """Generar prompt din√°mico basado en distribuci√≥n actual con COHERENCIA TOTAL"""

    # Obtener categor√≠as favorecidas
    industry_weights = distribution_tracker.get_distribution_weights('industries')
    seniority_weights = distribution_tracker.get_distribution_weights('seniorities')

    # Crear listas ordenadas por peso (descendente)
    favored_industries = sorted(industry_weights.keys(),
                               key=lambda x: industry_weights[x],
                               reverse=True)[:5]  # Top 5

    favored_seniorities = sorted(seniority_weights.keys(),
                                key=lambda x: seniority_weights[x],
                                reverse=True)[:4]  # Top 4

    # Mapas de coherencia por industria
    industry_requirements = {
        'Tecnolog√≠a': {
            'universities': ['UNAM (Ingenier√≠a en Sistemas)', 'IPN (Ingenier√≠a en Computaci√≥n)', 'ITESM (Ingenier√≠a en Tecnolog√≠as de Informaci√≥n)', 'UAM (Ingenier√≠a en Software)', 'UACM (Ciencia de Datos para Negocios)'],
            'degrees': ['Ingenier√≠a en Sistemas Computacionales', 'Ingenier√≠a en Software', 'Licenciatura en Inform√°tica', 'Ingenier√≠a en Tecnolog√≠as de la Informaci√≥n', 'Licenciatura en Ciencia de Datos para Negocios'],
            'skills': ['Python', 'JavaScript', 'Java', 'React', 'Node.js', 'SQL', 'AWS', 'Docker', 'Git', 'Machine Learning', 'DevOps', 'R', 'TensorFlow', 'Pandas', 'NumPy', 'Tableau', 'Power BI'],
            'experience_years': {'Junior': '1-3', 'Mid-Level': '3-5', 'Senior': '5-8', 'Lead': '8-12', 'Manager': '10-15', 'Director': '15+'},
            'companies': ['Tech Solutions', 'Digital Innovation', 'Software House', 'Cloud Systems', 'DataTech', 'WebDev Corp', 'Data Analytics Corp', 'Business Intelligence Solutions'],
            'positions': {'Junior': ['Desarrollador Junior', 'Analista de Sistemas', 'Analista de Datos Junior'], 'Mid-Level': ['Desarrollador Senior', 'Ingeniero de Software', 'Cient√≠fico de Datos'], 'Senior': ['Senior Developer', 'Tech Lead', 'Senior Data Scientist'], 'Lead': ['Lead Developer', 'Arquitecto de Software', 'Lead Data Scientist'], 'Manager': ['Gerente de Desarrollo', 'Scrum Master', 'Gerente de Analytics'], 'Director': ['Director de Tecnolog√≠a', 'CTO', 'Director de Data Science']}
        },
        'Ciencia de Datos': {
            'universities': ['UACM (Ciencia de Datos para Negocios)', 'UNAM (Matem√°ticas Aplicadas)', 'IPN (Estad√≠stica)', 'ITESM (Tecnolog√≠as de Informaci√≥n)', 'UDLAP (Negocios Digitales)'],
            'degrees': ['Licenciatura en Ciencia de Datos para Negocios', 'Licenciatura en Matem√°ticas Aplicadas', 'Licenciatura en Estad√≠stica', 'Ingenier√≠a en Datos', 'Licenciatura en Analytics'],
            'skills': ['Python', 'R', 'SQL', 'Machine Learning', 'Deep Learning', 'Big Data', 'Hadoop', 'Spark', 'TensorFlow', 'Pandas', 'NumPy', 'Scikit-learn', 'Tableau', 'Power BI', 'Excel Avanzado', 'A/B Testing', 'An√°lisis Predictivo', 'NLP', 'Computer Vision', 'Time Series Analysis'],
            'experience_years': {'Junior': '1-3', 'Mid-Level': '3-5', 'Senior': '5-8', 'Lead': '8-12', 'Manager': '10-15', 'Director': '15+'},
            'companies': ['Data Analytics Corp', 'Business Intelligence Solutions', 'Predictive Analytics Inc', 'Big Data Systems', 'AI Solutions', 'Data-Driven Consulting', 'Analytics Partners'],
            'positions': {'Junior': ['Analista de Datos Junior', 'Data Analyst I', 'Business Analyst'], 'Mid-Level': ['Cient√≠fico de Datos', 'Data Engineer', 'Analytics Manager'], 'Senior': ['Senior Data Scientist', 'Lead Analyst', 'Data Architect'], 'Lead': ['Principal Data Scientist', 'Head of Analytics', 'Data Science Lead'], 'Manager': ['Gerente de Data Science', 'Director de Analytics', 'Chief Data Officer'], 'Director': ['Director de Data Science', 'VP of Analytics', 'Chief Analytics Officer']}
        },
        'Finanzas': {
            'universities': ['UNAM (Econom√≠a)', 'ITESM (Finanzas)', 'UDLAP (Administraci√≥n)', 'UAM (Contadur√≠a)'],
            'degrees': ['Licenciatura en Econom√≠a', 'Licenciatura en Finanzas', 'Contadur√≠a P√∫blica', 'Administraci√≥n de Empresas'],
            'skills': ['Excel Avanzado', 'SQL', 'Python para Finanzas', 'Power BI', 'SAP', 'An√°lisis Financiero', 'Modelado Riesgos', 'Banca Digital'],
            'experience_years': {'Junior': '1-3', 'Mid-Level': '3-6', 'Senior': '6-10', 'Lead': '10-15', 'Manager': '12-18', 'Director': '18+'},
            'companies': ['Banco Nacional', 'Finanzas Globales', 'Investment Corp', 'Capital Advisors', 'Asset Management', 'Banca Digital'],
            'positions': {'Junior': ['Analista Financiero Junior', 'Asistente Contable'], 'Mid-Level': ['Analista Senior', 'Contador'], 'Senior': ['Senior Financial Analyst', 'Gerente de Riesgos'], 'Lead': ['Lead Analyst', 'Controller'], 'Manager': ['Gerente Financiero', 'Director de Finanzas'], 'Director': ['Director Financiero', 'CFO']}
        },
        'Salud': {
            'universities': ['UNAM (Medicina)', 'BUAP (Enfermer√≠a)', 'UDLAP (Psicolog√≠a)', 'UASLP (Fisioterapia)'],
            'degrees': ['Medicina', 'Enfermer√≠a', 'Psicolog√≠a', 'Fisioterapia', 'Nutrici√≥n', 'Farmacia'],
            'skills': ['Atenci√≥n al Paciente', 'Diagn√≥stico M√©dico', 'Gesti√≥n Hospitalaria', 'Sistemas de Salud', 'Epidemiolog√≠a', 'Telemedicina'],
            'experience_years': {'Junior': '1-3', 'Mid-Level': '3-6', 'Senior': '6-10', 'Lead': '10-15', 'Manager': '12-18', 'Director': '18+'},
            'companies': ['Hospital Central', 'Cl√≠nica Universitaria', 'Centro M√©dico', 'Instituto de Salud', 'Hospital General', 'Cl√≠nica Especializada'],
            'positions': {'Junior': ['Enfermero/a', 'T√©cnico M√©dico'], 'Mid-Level': ['Enfermero/a Senior', 'Especialista M√©dico'], 'Senior': ['Supervisor M√©dico', 'Coordinador de √Årea'], 'Lead': ['Jefe de Servicio', 'Especialista Senior'], 'Manager': ['Gerente M√©dico', 'Director de Departamento'], 'Director': ['Director M√©dico', 'Director General']}
        },
        'Marketing': {
            'universities': ['ITESM (Mercadotecnia)', 'UDLAP (Comunicaci√≥n)', 'UNAM (Publicidad)', 'UAM (Marketing Digital)'],
            'degrees': ['Licenciatura en Mercadotecnia', 'Comunicaci√≥n', 'Publicidad', 'Marketing Digital', 'Dise√±o Gr√°fico'],
            'skills': ['Google Analytics', 'SEO/SEM', 'Social Media', 'Adobe Creative Suite', 'Content Marketing', 'Brand Management', 'CRM'],
            'experience_years': {'Junior': '1-3', 'Mid-Level': '3-5', 'Senior': '5-8', 'Lead': '8-12', 'Manager': '10-15', 'Director': '15+'},
            'companies': ['Marketing Solutions', 'Brand Agency', 'Digital Media', 'Advertising Corp', 'Content Creators', 'E-commerce Marketing'],
            'positions': {'Junior': ['Ejecutivo de Cuentas', 'Asistente de Marketing'], 'Mid-Level': ['Especialista en Marketing', 'Coordinador Digital'], 'Senior': ['Senior Marketing Manager', 'Brand Manager'], 'Lead': ['Lead Marketing', 'Director de Marca'], 'Manager': ['Gerente de Marketing', 'Director de Campa√±as'], 'Director': ['Director de Marketing', 'CMO']}
        }
    }

    # Seleccionar seniority e industry espec√≠ficos para favorecer
    selected_seniority = distribution_tracker.get_weighted_choice('seniorities')
    selected_industry = distribution_tracker.get_weighted_choice('industries')

    # Mapas de experiencia por seniority
    experience_map = {
        'Junior': '2 a√±os',
        'Mid-Level': '4 a√±os', 
        'Senior': '7 a√±os',
        'Lead': '10 a√±os',
        'Manager': '15 a√±os',
        'Director': '20 a√±os'
    }

    # Mapas de skills por industria
    skills_map = {
        'Tecnolog√≠a': [
            'Python', 'JavaScript', 'Java', 'C++', 'React', 'Node.js', 'Angular', 'Vue.js',
            'SQL', 'NoSQL', 'MongoDB', 'PostgreSQL', 'MySQL', 'Redis',
            'AWS', 'Azure', 'Google Cloud', 'Docker', 'Kubernetes', 'Jenkins', 'Git',
            'Microservicios', 'API REST', 'GraphQL', 'DevOps', 'CI/CD', 'Linux', 'Bash'
        ],
        'Ciencia de Datos': [
            # Lenguajes de Programaci√≥n
            'Python', 'R', 'SQL', 'Julia', 'Scala', 'SAS', 'MATLAB',
            # Librer√≠as de Python/R
            'Pandas', 'NumPy', 'Scikit-learn', 'TensorFlow', 'PyTorch', 'Keras', 'XGBoost',
            'LightGBM', 'CatBoost', 'NLTK', 'spaCy', 'Transformers', 'OpenCV',
            # Visualizaci√≥n
            'Tableau', 'Power BI', 'matplotlib', 'seaborn', 'plotly', 'ggplot2', 'D3.js',
            'Looker', 'Qlik Sense', 'MicroStrategy',
            # Big Data
            'Hadoop', 'Spark', 'Kafka', 'Airflow', 'Databricks', 'Snowflake', 'Redshift',
            # Bases de Datos
            'PostgreSQL', 'MongoDB', 'Cassandra', 'Elasticsearch', 'Redis', 'Neo4j',
            # Cloud y MLOps
            'AWS SageMaker', 'Azure ML', 'Google AI Platform', 'MLflow', 'Kubeflow',
            # Estad√≠stica y Matem√°ticas
            'Estad√≠stica', 'Probabilidad', 'Machine Learning', 'Deep Learning', 'Time Series',
            'A/B Testing', 'An√°lisis Predictivo', 'Regresi√≥n', 'Clustering', 'NLP',
            # Business Intelligence
            'Business Intelligence', 'Data Warehousing', 'ETL', 'Data Mining', 'KPI',
            'Dashboarding', 'Storytelling con Datos', 'Data Governance', '√âtica en Datos'
        ],
        'Finanzas': [
            'Excel Avanzado', 'VBA', 'SQL', 'Python para Finanzas', 'R para Finanzas',
            'Power BI', 'Tableau', 'SAP', 'Oracle Financials', 'QuickBooks',
            'An√°lisis Financiero', 'Modelado de Riesgos', 'Valoraci√≥n de Activos',
            'Derivados', 'Forex', 'Banca Digital', 'FinTech', 'Blockchain', 'Criptomonedas',
            'Compliance', 'Auditor√≠a', 'Contabilidad', 'Impuestos', 'Mergers & Acquisitions'
        ],
        'Salud': [
            'Atenci√≥n al Paciente', 'Diagn√≥stico M√©dico', 'Gesti√≥n Hospitalaria',
            'Sistemas de Salud', 'Epidemiolog√≠a', 'Telemedicina', 'EHR', 'HL7', 'FHIR',
            'Investigaci√≥n Cl√≠nica', 'Ensayos Cl√≠nicos', 'Farmacolog√≠a', 'Gen√©tica',
            'Medicina Preventiva', 'Salud P√∫blica', 'Bioestad√≠stica', 'Data Analytics en Salud'
        ],
        'Marketing': [
            'Google Analytics', 'Google Ads', 'Facebook Ads', 'SEO/SEM', 'Content Marketing',
            'Social Media Marketing', 'Email Marketing', 'CRM', 'HubSpot', 'Salesforce',
            'Adobe Creative Suite', 'Photoshop', 'Illustrator', 'Brand Management',
            'Customer Journey', 'A/B Testing', 'Conversion Optimization', 'Growth Hacking',
            'Influencer Marketing', 'Marketing Automation', 'Data-Driven Marketing'
        ]
    }

    # Mapas de carreras por industria
    degree_map = {
        'Tecnolog√≠a': 'Ingenier√≠a en Sistemas',
        'Ciencia de Datos': 'Licenciatura en Ciencia de Datos para Negocios',
        'Finanzas': 'Licenciatura en Econom√≠a',
        'Salud': 'Medicina',
        'Marketing': 'Licenciatura en Mercadotecnia'
    }

    # Prompt Maestro ULTRA-ESPEC√çFICO
    prompt = f"""
Genera 1 perfil profesional mexicano en JSON puro.

DATOS ESPEC√çFICOS (USA EXACTAMENTE ESTOS VALORES):
- Industria: {selected_industry}
- Seniority: {selected_seniority}
- Experiencia: {experience_map[selected_seniority]}
- Skills: {', '.join(skills_map.get(selected_industry, ['Gen√©ricas'])[:3])}
- Universidad: UNAM
- Carrera: {degree_map.get(selected_industry, 'Profesional')}
- Ubicaci√≥n: Ciudad de M√©xico
- Idiomas: Espa√±ol, Ingl√©s

INSTRUCCIONES:
- El cv_text DEBE mencionar exactamente "{experience_map[selected_seniority]}" de experiencia
- Los annotations.experience DEBEN tener exactamente {experience_map[selected_seniority].split()[0]} entradas (una por a√±o)
- NO cambies los valores especificados arriba

FORMATO JSON EXACTO:
{{
  "metadata": {{"industry": "{selected_industry}", "seniority": "{selected_seniority}"}},
  "cv_text": "NOMBRE: Juan P√©rez\\nEXPERIENCIA: {experience_map[selected_seniority]} en {selected_industry}\\nHABILIDADES: {', '.join(skills_map.get(selected_industry, ['Gen√©ricas'])[:3])}\\n...",
  "annotations": {{
    "name": "Juan P√©rez",
    "education": [{{"institution": "UNAM", "degree": "{degree_map.get(selected_industry, 'Profesional')}"}}],
    "experience": [{{"position": "{selected_seniority} {selected_industry}", "company": "Empresa Mexicana"}}],
    "skills": {skills_map.get(selected_industry, ['Gen√©ricas'])[:3]},
    "location": "Ciudad de M√©xico",
    "languages": ["Espa√±ol", "Ingl√©s"]
  }}
}}
"""

    return prompt

def generate_batch_profiles():
    """Generar lote de 5 perfiles usando el Prompt Maestro del otro LLM"""
    try:
        # Obtener todas las industrias y seniorities del target distribution
        all_industries = list(TARGET_DISTRIBUTION['industries'].keys())
        all_seniorities = list(TARGET_DISTRIBUTION['seniorities'].keys())

        # Prompt Maestro del otro LLM (adaptado con TODAS las categor√≠as)
        prompt_maestro = f"""
Genera 5 objetos JSON √∫nicos. Cada objeto debe representar un perfil profesional completamente ficticio y distinto (diferente industria, nivel de seniority, g√©nero, nacionalidad y universidad).

Reglas de Estocasticidad (Variabilidad):
1. Industria: Elige aleatoriamente entre {', '.join(all_industries)}.
2. Nivel: Var√≠a entre {', '.join(all_seniorities)}.
3. Estilo: El campo cv_text debe ser un Curr√≠culum en "Estilo Harvard" (texto plano, sobrio, orientado a logros, sin columnas, uso de bullet points).
4. Idioma: Espa√±ol.
5. Especializaci√≥n: Para Ciencia de Datos, enfatizar habilidades t√©cnicas como Python, R, Machine Learning, an√°lisis predictivo y visualizaci√≥n de datos.

Formato de Salida (JSON Array estricto):
No incluyas texto introductorio ni markdown (```json). Devuelve SOLO la lista de objetos con esta estructura exacta:

[
  {{
    "metadata": {{
      "industry": "Finanzas",
      "seniority": "Senior",
      "profile_id": "uuid_simulado"
    }},
    "cv_text": "NOMBRE: Ana L√≥pez\\nTEL√âFONO: 555-0199\\nEMAIL: ana.lopez@email.com\\n\\nEXPERIENCIA PROFESIONAL\\n\\nCIENT√çFICO DE DATOS SENIOR | PREDICTIVE ANALYTICS INC | 2019 - PRESENTE\\n- Desarroll√© modelos de machine learning que aumentaron la precisi√≥n predictiva en un 35%.\\n- Implement√© pipelines de datos automatizados procesando 10TB de datos diarios.\\n- Lider√© equipo de 4 data scientists en proyectos de an√°lisis predictivo.\\n\\nEDUCACI√ìN\\n\\nLICENCIATURA EN CIENCIA DE DATOS PARA NEGOCIOS | UACM | 2015 - 2019\\n- Graduada con menci√≥n honor√≠fica.\\n- Proyecto final: Sistema de recomendaci√≥n basado en machine learning.",
    "annotations": {{
      "name": "Ana L√≥pez",
      "email": "ana.lopez@email.com",
      "current_role": "Cient√≠fico de Datos Senior",
      "years_experience": 7,
      "degree": "Licenciatura en Ciencia de Datos para Negocios",
      "university": "UACM",
      "skills": ["Python", "Machine Learning", "SQL", "Tableau", "TensorFlow"]
    }}
  }}
]
"""

        response = client.chat.completions.create(
            model=MODEL_ID,
            messages=[
                {"role": "system", "content": "Eres un generador de datos sint√©ticos experto en Recursos Humanos y NLP. Tu tarea es generar datos de entrenamiento de alta variabilidad."},
                {"role": "user", "content": prompt_maestro}
            ],
            temperature=0.9,  # ALTO para m√°xima creatividad/estocasticidad
            max_tokens=2000,
            timeout=120,
            stream=False
        )

        content = response.choices[0].message.content
        content = content.replace("```json", "").replace("```", "").strip()

        try:
            batch = json.loads(content)
            if isinstance(batch, list):
                return batch
            else:
                return [batch]  # Si devuelve un solo objeto
        except json.JSONDecodeError as e:
            print(f"‚ùå Error decodificando JSON del lote: {e}")
            print(f"Contenido: {content[:300]}...")
            return []

    except Exception as e:
        print(f"‚ùå Error generando lote: {e}")
        return []

def load_existing_distribution():
    """Cargar distribuci√≥n existente desde la base de datos"""
    try:
        cursor.execute("SELECT industry, seniority FROM cv_dataset")
        rows = cursor.fetchall()

        for industry, seniority in rows:
            if industry and industry != 'Unknown':
                distribution_tracker.update('industries', industry)
            if seniority and seniority != 'Unknown':
                distribution_tracker.update('seniorities', seniority)

        print(f"üìä Cargada distribuci√≥n existente: {len(rows)} registros")
        distribution_tracker.print_balance_report()

    except Exception as e:
        print(f"‚ö†Ô∏è Error cargando distribuci√≥n existente: {e}")

def generate_profile():
    try:
        # Generar prompt din√°mico basado en distribuci√≥n actual
        current_prompt = generate_dynamic_prompt()

        print(f"‚è≥ Solicitando perfil al modelo {MODEL_ID}...")
        start_time = time.time()

        response = client.chat.completions.create(
            model=MODEL_ID,
            messages=[
                {"role": "system", "content": current_prompt}
            ],
            temperature=0.8,  # Reducido para m√°s consistencia
            max_tokens=1200,  # Reducido para modelo local
            timeout=60,
            stream=False
        )

        duration = time.time() - start_time
        print(f"‚úÖ Respuesta recibida en {duration:.2f}s")

        content = response.choices[0].message.content
        content = content.replace("```json", "").replace("```", "").strip()

        try:
            data = json.loads(content)
            # Si el modelo devuelve lista por error, tomamos el primero
            if isinstance(data, list):
                return data[0] if data else None
            return data
        except json.JSONDecodeError as e:
            print("‚ùå Error decodificando JSON.")
            print(f"Contenido recibido: {content[:500]}...")  # Mostrar primeros 500 caracteres
            print(f"Error espec√≠fico: {e}")
            return None

    except Exception as e:
        print(f"‚ùå Error generando perfil: {e}")
        return None

def validate_profile_coherence(profile):
    """Validar que todos los atributos del perfil sean coherentes entre s√≠"""
    issues = []

    try:
        metadata = profile.get('metadata', {})
        annotations = profile.get('annotations', {})

        # Buscar industry y seniority en metadata primero, luego en annotations (para compatibilidad con perfiles antiguos)
        industry = metadata.get('industry') or annotations.get('industry', '')
        seniority = metadata.get('seniority') or annotations.get('seniority', '')

        if not industry or not seniority:
            issues.append("Faltan industry o seniority en metadata/annotations")
            return issues

        # 1. Validar educaci√≥n
        education = annotations.get('education', [])
        if education:
            edu = education[0]  # Tomar primera educaci√≥n
            institution = edu.get('institution', '').lower()
            degree = edu.get('degree', '').lower()

            # Verificar universidades mexicanas
            mexican_universities = ['unam', 'ipn', 'uam', 'itesm', 'udlap', 'buap', 'uaslp', 'udem']
            if not any(uni in institution for uni in mexican_universities):
                issues.append(f"Universidad no mexicana: {institution}")

            # Verificar coherencia carrera-industria
            industry_keywords = {
                'tecnolog√≠a': ['ingenier√≠a', 'sistemas', 'computaci√≥n', 'software', 'inform√°tica'],
                'ciencia de datos': ['ciencia de datos', 'matem√°ticas aplicadas', 'estad√≠stica', 'analytics', 'datos'],
                'finanzas': ['econom√≠a', 'finanzas', 'contadur√≠a', 'administraci√≥n', 'negocios'],
                'salud': ['medicina', 'enfermer√≠a', 'psicolog√≠a', 'fisioterapia', 'nutrici√≥n'],
                'marketing': ['mercadotecnia', 'comunicaci√≥n', 'publicidad', 'marketing', 'dise√±o']
            }

            industry_lower = industry.lower()
            if industry_lower in industry_keywords:
                if not any(keyword in degree for keyword in industry_keywords[industry_lower]):
                    issues.append(f"Carrera '{degree}' no coherente con industria '{industry}'")

        # 2. Validar experiencia
        experience = annotations.get('experience', [])
        total_years = len(experience)

        # Para perfiles generados con el nuevo sistema, esperamos experiencia espec√≠fica
        expected_years = {
            'Junior': 2,
            'Mid-Level': 4,
            'Senior': 7,
            'Lead': 10,
            'Manager': 15,
            'Director': 20
        }

        seniority_lower = seniority.lower()
        if seniority in expected_years:
            expected = expected_years[seniority]
            if total_years != expected:
                issues.append(f"Experiencia ({total_years} a√±os) no coherente con seniority '{seniority}' (esperado: {expected})")
        else:
            # Fallback para seniority no mapeado
            seniority_years = {
                'junior': (0, 3),
                'mid-level': (3, 6),
                'senior': (5, 10),
                'lead': (8, 15),
                'manager': (10, 20),
                'director': (15, 30)
            }
            if seniority_lower in seniority_years:
                min_years, max_years = seniority_years[seniority_lower]
                if total_years < min_years or total_years > max_years:
                    issues.append(f"Experiencia ({total_years} a√±os) no coherente con seniority '{seniority}' (esperado: {min_years}-{max_years})")

        # 3. Validar habilidades t√©cnicas
        skills = annotations.get('skills', [])
        if skills:
            # Verificar que las habilidades sean t√©cnicas y relevantes
            industry_tech_skills = {
                'tecnolog√≠a': ['python', 'javascript', 'java', 'c++', 'react', 'node.js', 'angular', 'vue.js', 'sql', 'nosql', 'mongodb', 'postgresql', 'mysql', 'redis', 'aws', 'azure', 'google cloud', 'docker', 'kubernetes', 'jenkins', 'git', 'microservicios', 'api rest', 'graphql', 'devops', 'ci/cd', 'linux', 'bash'],
                'ciencia de datos': ['python', 'r', 'sql', 'julia', 'scala', 'sas', 'matlab', 'pandas', 'numpy', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'xgboost', 'lightgbm', 'catboost', 'nltk', 'spacy', 'transformers', 'opencv', 'tableau', 'power bi', 'matplotlib', 'seaborn', 'plotly', 'ggplot2', 'd3.js', 'looker', 'qlik sense', 'microstrategy', 'hadoop', 'spark', 'kafka', 'airflow', 'databricks', 'snowflake', 'redshift', 'postgresql', 'mongodb', 'cassandra', 'elasticsearch', 'redis', 'neo4j', 'aws sagemaker', 'azure ml', 'google ai platform', 'mlflow', 'kubeflow', 'estad√≠stica', 'probabilidad', 'machine learning', 'deep learning', 'time series', 'a/b testing', 'an√°lisis predictivo', 'regresi√≥n', 'clustering', 'nlp', 'business intelligence', 'data warehousing', 'etl', 'data mining', 'kpi', 'dashboarding', 'storytelling con datos', 'data governance', '√©tica en datos'],
                'finanzas': ['excel', 'vba', 'sql', 'python', 'r', 'power bi', 'tableau', 'sap', 'oracle financials', 'quickbooks', 'an√°lisis financiero', 'modelado de riesgos', 'valoraci√≥n de activos', 'derivados', 'forex', 'banca digital', 'fintech', 'blockchain', 'criptomonedas', 'compliance', 'auditor√≠a', 'contabilidad', 'impuestos', 'mergers & acquisitions'],
                'salud': ['atenci√≥n al paciente', 'diagn√≥stico m√©dico', 'gesti√≥n hospitalaria', 'sistemas de salud', 'epidemiolog√≠a', 'telemedicina', 'ehr', 'hl7', 'fhir', 'investigaci√≥n cl√≠nica', 'ensayos cl√≠nicos', 'farmacolog√≠a', 'gen√©tica', 'medicina preventiva', 'salud p√∫blica', 'bioestad√≠stica', 'data analytics en salud'],
                'marketing': ['google analytics', 'google ads', 'facebook ads', 'seo/sem', 'content marketing', 'social media marketing', 'email marketing', 'crm', 'hubspot', 'salesforce', 'adobe creative suite', 'photoshop', 'illustrator', 'brand management', 'customer journey', 'a/b testing', 'conversion optimization', 'growth hacking', 'influencer marketing', 'marketing automation', 'data-driven marketing']
            }

            industry_lower = industry.lower()
            if industry_lower in industry_tech_skills:
                relevant_skills = industry_tech_skills[industry_lower]
                matching_skills = [skill for skill in skills if any(rel_skill.lower() in skill.lower() for rel_skill in relevant_skills)]
                if len(matching_skills) < len(skills) * 0.5:  # Al menos 50% de skills relevantes
                    issues.append(f"Pocas habilidades t√©cnicas relevantes para '{industry}': {matching_skills}")

        # 4. Validar ubicaci√≥n
        location = annotations.get('location', '').lower()
        mexican_cities = ['ciudad de m√©xico', 'guadalajara', 'monterrey', 'puebla', 'tijuana', 'm√©rida', 'le√≥n', 'quer√©taro', 'mexico city']
        if location and not any(city in location for city in mexican_cities):
            issues.append(f"Ubicaci√≥n no mexicana: {location}")

        # 5. Validar idiomas
        languages = annotations.get('languages', [])
        if languages:
            has_spanish = any('espa√±ol' in lang.lower() for lang in languages)
            if not has_spanish:
                issues.append("Falta espa√±ol como idioma nativo")

            # Verificar nivel de ingl√©s seg√∫n seniority
            has_english = any('ingl√©s' in lang.lower() for lang in languages)
            if seniority_lower in ['senior', 'lead', 'manager', 'director'] and not has_english:
                issues.append(f"Seniority '{seniority}' deber√≠a tener ingl√©s")

    except Exception as e:
        issues.append(f"Error en validaci√≥n: {str(e)}")

    return issues

def main():
    # Bucle infinito (o hasta llegar a N)
    # OBJETIVO definido globalmente

    # Cargar distribuci√≥n existente
    load_existing_distribution()

    # Verificar cu√°ntos tenemos ya
    cursor.execute("SELECT COUNT(*) FROM cv_dataset")
    total_generados = cursor.fetchone()[0]

    print(f"üöÄ Iniciando miner√≠a de CVs sint√©ticos (Modo Balanceado con Validaci√≥n de Coherencia).")
    print(f"üéØ Objetivo: {OBJETIVO}")
    print(f"üìä Actual: {total_generados}")
    print(f"ü§ñ Modelo: {MODEL_ID} @ {API_BASE_URL}")
    print("-" * 50)

    report_interval = 25  # Mostrar reporte cada 25 CVs

    while total_generados < OBJETIVO:
        item = generate_profile()

        if not item:
            print("‚ö†Ô∏è Fallo en generaci√≥n. Reintentando en 2s...")
            time.sleep(2)
            continue

        try:
            unique_id = str(uuid.uuid4())

            # Validar campos m√≠nimos
            if 'cv_text' not in item or 'annotations' not in item:
                print("‚ö†Ô∏è JSON incompleto, saltando...")
                continue

            # VALIDAR COHERENCIA DEL PERFIL
            coherence_issues = validate_profile_coherence(item)
            if coherence_issues:
                print(f"‚ö†Ô∏è Perfil incoherente ({len(coherence_issues)} problemas), reintentando...")
                for issue in coherence_issues[:2]:  # Mostrar m√°ximo 2 problemas
                    print(f"   ‚Ä¢ {issue}")
                continue  # Reintentar con nuevo perfil

            # Extraer metadata para tracking - CORREGIDO para usar metadata
            industry = item.get('metadata', {}).get('industry', 'Unknown')
            seniority = item.get('metadata', {}).get('seniority', 'Unknown')

            # Tambi√©n actualizar annotations con industry/seniority para consistencia
            if 'annotations' in item:
                item['annotations']['industry'] = industry
                item['annotations']['seniority'] = seniority

            cursor.execute('''
                INSERT INTO cv_dataset (id, industry, seniority, cv_text, annotations)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                unique_id,
                industry,
                seniority,
                item['cv_text'],
                json.dumps(item['annotations'])
            ))

            conn.commit()

            # Actualizar distribuci√≥n
            distribution_tracker.update('industries', industry)
            distribution_tracker.update('seniorities', seniority)

            total_generados += 1

            # Reporte peri√≥dico
            if total_generados % report_interval == 0:
                print(f"\nüìä Progreso: {total_generados}/{OBJETIVO} CVs generados")
                distribution_tracker.print_balance_report()
                print("-" * 50)

            print(f"üíæ Guardado 1 CV ({industry}/{seniority}). Progreso total: {total_generados}/{OBJETIVO}")

        except Exception as e:
            print(f"Error insertando item: {e}")

        # Pausa breve para dejar respirar al servidor local
        time.sleep(0.1)

    # Reporte final
    print(f"\nüéâ ¬°Entrenamiento completado! Base de datos finalizada con {total_generados} CVs.")
    distribution_tracker.print_balance_report()

    conn.close()

def validate_distribution():
    """Funci√≥n para validar la distribuci√≥n actual de la base de datos"""
    try:
        # Cargar datos actuales
        load_existing_distribution()

        # Obtener estad√≠sticas
        stats = distribution_tracker.get_stats()

        print("üîç VALIDACI√ìN DE DISTRIBUCI√ìN EN BASE DE DATOS")
        print("=" * 60)

        total_issues = 0

        for category, subcats in stats.items():
            print(f"\nüìä {category.upper()}:")
            issues = 0

            for subcat, data in subcats.items():
                deviation = abs(data['percentage'] - data['target'])
                if deviation > 10:  # M√°s de 10% de desviaci√≥n
                    status = "‚ùå CR√çTICO"
                    issues += 1
                elif deviation > 5:  # M√°s de 5% de desviaci√≥n
                    status = "‚ö†Ô∏è  ALTO"
                    issues += 1
                else:
                    status = "‚úÖ OK"

                print(f"  {status} {subcat}: {data['count']} ({data['percentage']:.1f}% | Target: {data['target']:.1f}%)")

            if issues > 0:
                print(f"  üî¥ {issues} subcategor√≠as con desviaciones significativas")
                total_issues += issues
            else:
                print(f"  ‚úÖ Distribuci√≥n balanceada")

        print(f"\n" + "=" * 60)
        if total_issues == 0:
            print("üéâ ¬°DISTRIBUCI√ìN PERFECTA! Dataset listo para entrenamiento.")
        elif total_issues < 5:
            print(f"‚ö†Ô∏è  Distribuci√≥n aceptable con {total_issues} desviaciones menores.")
        else:
            print(f"‚ùå Distribuci√≥n requiere rebalanceo. {total_issues} problemas detectados.")
            print("üí° Recomendaci√≥n: Ejecutar generaci√≥n adicional con el sistema de balance.")

    except Exception as e:
        print(f"‚ùå Error en validaci√≥n: {e}")

def validate_profile_coherence_batch():
    """Validar coherencia de todos los perfiles en la base de datos"""
    try:
        cursor.execute("SELECT industry, seniority, annotations FROM cv_dataset")
        rows = cursor.fetchall()

        print("üîç VALIDACI√ìN DE COHERENCIA DE PERFILES")
        print("=" * 60)

        total_profiles = len(rows)
        profiles_with_issues = 0
        total_issues = 0

        for i, row in enumerate(rows):
            try:
                industry_db, seniority_db, annotations_json = row
                profile = json.loads(annotations_json)
                
                # Reconstruir estructura completa con metadata de DB para perfiles antiguos
                full_profile = {
                    'metadata': {
                        'industry': industry_db,
                        'seniority': seniority_db
                    },
                    'annotations': profile
                }

                issues = validate_profile_coherence(full_profile)
                if issues:
                    profiles_with_issues += 1
                    total_issues += len(issues)
                    if i < 5:  # Mostrar primeros 5 con problemas
                        print(f"\n‚ùå Perfil {i+1} - {len(issues)} problemas:")
                        for issue in issues[:3]:  # M√°ximo 3 problemas por perfil
                            print(f"   ‚Ä¢ {issue}")

            except Exception as e:
                print(f"Error procesando perfil {i+1}: {e}")

        print(f"\n" + "=" * 60)
        print(f"üìä Resumen de Validaci√≥n:")
        print(f"   ‚Ä¢ Total de perfiles: {total_profiles}")
        if total_profiles > 0:
            print(f"   ‚Ä¢ Perfiles con problemas: {profiles_with_issues} ({profiles_with_issues/total_profiles*100:.1f}%)")
        else:
            print(f"   ‚Ä¢ Perfiles con problemas: {profiles_with_issues} (base de datos vac√≠a)")
        print(f"   ‚Ä¢ Total de problemas: {total_issues}")

        if profiles_with_issues == 0:
            print("üéâ ¬°TODOS los perfiles son coherentes!")
        elif total_profiles > 0 and profiles_with_issues / total_profiles < 0.1:
            print("‚úÖ Coherencia aceptable - pocos perfiles con problemas menores")
        else:
            print("‚ùå Muchos perfiles con problemas de coherencia - revisar prompt")

    except Exception as e:
        print(f"‚ùå Error en validaci√≥n de coherencia: {e}")

def batch_generate_main():
    """Funci√≥n principal para generar CVs en lotes usando el Prompt Maestro"""
    # Cargar distribuci√≥n existente
    load_existing_distribution()

    # Verificar cu√°ntos tenemos ya
    cursor.execute("SELECT COUNT(*) FROM cv_dataset")
    total_generados = cursor.fetchone()[0]

    print(f"üöÄ Iniciando generaci√≥n en LOTES (Prompt Maestro Experimental).")
    print(f"üéØ Objetivo: {OBJETIVO}")
    print(f"üìä Actual: {total_generados}")
    print(f"ü§ñ Modelo: {MODEL_ID} @ {API_BASE_URL}")
    print("-" * 50)

    while total_generados < OBJETIVO:
        batch = generate_batch_profiles()

        if not batch:
            print("‚ö†Ô∏è Fallo en generaci√≥n del lote. Reintentando en 2s...")
            time.sleep(2)
            continue

        batch_size = len(batch)
        print(f"üì¶ Procesando lote de {batch_size} CVs...")

        for item in batch:
            if total_generados >= OBJETIVO:
                break

            try:
                unique_id = str(uuid.uuid4())

                # Validar campos m√≠nimos
                if 'cv_text' not in item or 'annotations' not in item or 'metadata' not in item:
                    print("‚ö†Ô∏è JSON incompleto en lote, saltando...")
                    continue

                # Extraer metadata
                industry = item['metadata'].get('industry', 'Unknown')
                seniority = item['metadata'].get('seniority', 'Unknown')

                # Actualizar annotations con industry/seniority para consistencia
                if 'annotations' in item:
                    item['annotations']['industry'] = industry
                    item['annotations']['seniority'] = seniority

                cursor.execute('''
                    INSERT INTO cv_dataset (id, industry, seniority, cv_text, annotations)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    unique_id,
                    industry,
                    seniority,
                    item['cv_text'],
                    json.dumps(item['annotations'])
                ))

                conn.commit()

                # Actualizar distribuci√≥n
                distribution_tracker.update('industries', industry)
                distribution_tracker.update('seniorities', seniority)

                total_generados += 1

                print(f"üíæ Guardado CV {total_generados}/{OBJETIVO} ({industry}/{seniority})")

            except Exception as e:
                print(f"Error insertando item del lote: {e}")

        # Pausa entre lotes
        time.sleep(1)

    # Reporte final
    print(f"\nüéâ ¬°Generaci√≥n en lotes completada! Base de datos finalizada con {total_generados} CVs.")
    distribution_tracker.print_balance_report()

    conn.close()

# Funci√≥n para ejecutar validaci√≥n si se llama con --validate
if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--validate":
        validate_distribution()
        print("\n" + "="*60)
        validate_profile_coherence_batch()
    elif len(sys.argv) > 1 and sys.argv[1] == "--clean":
        print("üßπ LIMPIANDO BASE DE DATOS...")
        cursor.execute("DELETE FROM cv_dataset")
        conn.commit()
        print("‚úÖ Base de datos limpiada. Todos los registros eliminados.")
        conn.close()
    elif len(sys.argv) > 1 and sys.argv[1] == "--batch":
        print("üîÑ GENERANDO CVs EN LOTES (MODO EXPERIMENTAL)")
        batch_generate_main()
    else:
        try:
            main()
        except KeyboardInterrupt:
            print("\nüõë Detenido por el usuario. Datos guardados.")
            distribution_tracker.print_balance_report()
            conn.close()
        except Exception as e:
            print(f"\n‚ùå Error fatal: {e}")
            distribution_tracker.print_balance_report()
            conn.close()
            sys.exit(1)
